# Industrial Workshop Optimization with Q-Learning  
*A reinforcement-learning environment modeling batch production under real-world constraints.*

This project implements a complete Q-Learning workflow for a simulated industrial workshop that must manage stock limits, production batches, time-dependent actions, and storage penalties.  
It is the fourth and most advanced iteration of the model, integrating all previously added constraints (stock, costs, time, penalties).

---

## 1. Environment Rules

The workshop operates with two bounded stock variables:

- **stock_raw** ‚Äî raw material (0 to 10)  
- **stock_sell** ‚Äî finished products (0 to 10)

The agent can perform **22 discrete actions**:

| Action | Description                                      |
|--------|--------------------------------------------------|
| 0      | Wait                                             |
| 1..10  | Produce k units of Product 1 (P1)                |
| 11..20 | Produce k units of Product 2 (P2)                |
| 21     | Order +5 units of raw material                   |

**Hard constraints:** both stock levels must remain within `[0, 10]`.

---

## 2. Costs and Rewards

### Product 1 (P1)
- Raw material cost: **1 unit per product**
- Profit: **+2 per unit**
- Duration: **1 time unit per product**

### Product 2 (P2)
- Raw material cost: **2 units per product**
- Profit: **+20 per unit**
- Duration: **3 time units per product**

### Raw Material Order (+5 MP)
- Reward: **‚Äì5**
- Duration: **1**

### Wait
- If `stock_raw = 0` ‚Üí **‚Äì1**
- Otherwise ‚Üí 0  
- Duration: **1**

### Storage Penalty
Applied after each action:

```
reward -= 0.5 * stock_sell
```

---

## 3. Time Management

Time directly affects rewards through action duration:

| Action        | Duration |
|---------------|----------|
| Wait          | 1        |
| Order         | 1        |
| Produce P1    | k        |
| Produce P2    | 3k       |

Each episode lasts **50 time units max**.

---

## 4. Q-Learning Training and Update Equation

### Q-Learning Update Equation

The Q-table is updated using the Bellman rule:

\[
Q(s,a) \leftarrow Q(s,a) + \alpha \left[ r + \gamma \, \max_{a'} Q(s',a') - Q(s,a) \right]
\]

Where:

- \(s\) = current state  
- \(a\) = chosen action  
- \(r\) = reward received  
- \(s'\) = next state  
- \(a'\) = possible next actions  
- \(\alpha\) = learning rate  
- \(\gamma\) = discount factor  

This rule allows the agent to estimate long-term value by integrating both immediate rewards and future opportunities.

### Hyperparameters

- `alpha = 0.1`  
- `gamma = 0.95`  
- `epsilon_decay = 0.995`  
- `n_actions = 22`

The Q-table is defined as:

```
Q[stock_raw][stock_sell][action]
```

---

## 5. Optimal Policy (Learned Behavior)

- P2 is preferred when `stock_sell` is low  
- P1 regulates penalties when `stock_sell` is high  
- Ordering raw material occurs **only when stock_raw = 0**  
- Waiting appears in high-penalty or low-time-remaining states  

---

## 6. Business-Oriented Synthesis

### 1. Ordering Policy
The agent orders raw material **only when stock_raw = 0**.

### 2. Product 2 (P2) ‚Äî High-Profit Engine
When `stock_sell ‚â§ 3`:
- P2 dominates  
- Long batches maximize early-cycle profitability  

### 3. Product 1 (P1) ‚Äî Fine Regulation
When `stock_sell ‚â• 4`:
- P1 prevents excessive storage penalties  

### 4. Waiting
Used when:
- penalties would rise excessively  
- remaining time is too short  

### 5. Strategic Overview
- **Use P2 aggressively** early  
- **Use P1** to stabilize penalties  
- **Order** only with zero raw stock  
- **Wait** when necessary  

---

## 7. Source Notebook

Located in:

- `notebook/modelisation4.ipynb`

---

# üá´üá∑ Version Fran√ßaise ‚Äî Mod√©lisation 4 : Atelier Industriel avec Q-Learning

Ce projet impl√©mente un environnement complet de Reinforcement Learning simulant un atelier industriel soumis √† des contraintes r√©elles : limites de stock, production en lots, p√©nalit√©s de stockage, dur√©e variable des actions et arbitrages √©conomiques.

---

## 1. R√®gles de l‚Äôenvironnement

Deux stocks born√©s :

- **stock_raw** : mati√®re premi√®re (0 √† 10)  
- **stock_sell** : produits finis (0 √† 10)

Actions possibles (**22 actions**) :

| Action | Description                                      |
|--------|--------------------------------------------------|
| 0      | Attendre                                         |
| 1..10  | Produire k unit√©s de Produit 1 (P1)              |
| 11..20 | Produire k unit√©s de Produit 2 (P2)              |
| 21     | Commander +5 unit√©s de MP                        |

---

## 2. Co√ªts et r√©compenses

### Produit 1 (P1)
- Co√ªt MP : **1**  
- Marge : **+2**  
- Dur√©e : **1**

### Produit 2 (P2)
- Co√ªt MP : **2**  
- Marge : **+20**  
- Dur√©e : **3**

### Commande
- R√©compense : **‚Äì5**  
- Dur√©e : **1**

### Attente
- Si `stock_raw = 0` ‚Üí **‚Äì1**, sinon 0  

### P√©nalit√© de stockage
```
reward -= 0.5 * stock_sell
```

---

## 3. Gestion du temps

| Action      | Dur√©e |
|-------------|-------|
| Attendre    | 1     |
| Commander   | 1     |
| Produire P1 | k     |
| Produire P2 | 3k    |

Un √©pisode dure **50 unit√©s de temps**.

---

## 4. Entra√Ænement Q-Learning et √âquation de Mise √† Jour

### √âquation de Bellman

La Q-table est mise √† jour selon :

\[
Q(s,a) \leftarrow Q(s,a) + \alpha \left[ r + \gamma \, \max_{a'} Q(s',a') - Q(s,a) \right]
\]

Avec :

- \(s\) : √©tat courant  
- \(a\) : action effectu√©e  
- \(r\) : r√©compense re√ßue  
- \(s'\) : nouvel √©tat  
- \(a'\) : actions possibles  
- \(\alpha\) : taux d‚Äôapprentissage  
- \(\gamma\) : facteur de discount  

---

## 5. Politique optimale

- P2 privil√©gi√© lorsque `stock_sell` est bas  
- P1 en r√©gulation lorsque `stock_sell` est √©lev√©  
- Commande uniquement avec **stock_raw = 0**  
- Attente dans les zones √† forte p√©nalit√©  

---

## 6. Synth√®se m√©tier

- **P2** maximise le profit initial  
- **P1** stabilise les p√©nalit√©s  
- **Commande** uniquement en cas de p√©nurie  
- **Attente** lorsque produire serait n√©faste  

---

## 7. Notebook

Disponible dans :  
`notebook/modelisation4.ipynb`
