{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f9fba88-2afe-45b6-a2d5-352306500587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a5de7e8bd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cellule 1 — Imports de base et configuration\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "from env.workshop_env import WorkshopEnv  # ton environnement atelier\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a8cda84-fbd7-409d-80eb-89349e9e76be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espace d'observation : Box(0.0, [1.008e+04 1.000e+00 1.000e+02 1.000e+00 1.000e+02 5.000e+01 5.000e+01\n",
      " 5.000e+01 5.000e+01 1.008e+04 1.000e+03 1.000e+03 1.000e+03 4.000e+00\n",
      " 5.000e+01 2.000e+02 1.439e+03 2.000e+03 2.000e+04 1.000e+00 1.000e+06\n",
      " 1.000e+05 1.500e+01], (23,), float32)\n",
      "Espace d'actions     : Discrete(201)\n"
     ]
    }
   ],
   "source": [
    "# Cellule 2 — Environnement d'entraînement avec masque d'actions\n",
    "\n",
    "def mask_fn(env: WorkshopEnv):\n",
    "    \"\"\"\n",
    "    Fonction utilisée par ActionMasker pour récupérer le masque des actions.\n",
    "    \"\"\"\n",
    "    return env.get_action_mask()\n",
    "\n",
    "# Environnement enveloppé pour PPO\n",
    "env_train = ActionMasker(WorkshopEnv(), mask_fn)\n",
    "\n",
    "print(\"Espace d'observation :\", env_train.observation_space)\n",
    "print(\"Espace d'actions     :\", env_train.action_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcbda6a8-a5b0-40c0-8682-70b95f750513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Modèle initial chargé depuis student_dagger_final.zip\n",
      "\n",
      "=== Hyperparamètres PPO SAFE configurés ===\n",
      "learning_rate      = 1e-05\n",
      "clip_range         = constant schedule à 0.05\n",
      "ent_coef           = 0.001\n",
      "n_steps (conservé) = 4096\n",
      "batch_size (conservé) = 512\n",
      "================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabri\\anaconda3\\envs\\qlearning\\lib\\site-packages\\stable_baselines3\\common\\utils.py:168: UserWarning: get_schedule_fn() is deprecated, please use FloatSchedule() instead\n",
      "  warnings.warn(\"get_schedule_fn() is deprecated, please use FloatSchedule() instead\")\n",
      "C:\\Users\\fabri\\anaconda3\\envs\\qlearning\\lib\\site-packages\\stable_baselines3\\common\\utils.py:214: UserWarning: constant_fn() is deprecated, please use ConstantSchedule() instead\n",
      "  warnings.warn(\"constant_fn() is deprecated, please use ConstantSchedule() instead\")\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# Cellule 3 — Chargement modèle DAgger + réglages PPO SAFE\n",
    "# ======================================================\n",
    "\n",
    "from stable_baselines3.common.utils import get_schedule_fn\n",
    "\n",
    "MODEL_PATH = \"student_dagger_final.zip\"\n",
    "\n",
    "# Chargement du modèle issu de DAgger\n",
    "model_ppo = MaskablePPO.load(\n",
    "    MODEL_PATH,\n",
    "    env=env_train,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "print(\"Modèle initial chargé depuis\", MODEL_PATH)\n",
    "\n",
    "# ======================================================\n",
    "# Hyperparamètres SAFE pour un finetuning PPO NON destructif\n",
    "# (on NE TOUCHE PAS à n_steps ni batch_size, pour ne pas casser le buffer)\n",
    "# ======================================================\n",
    "\n",
    "# 1) Learning rate très faible : micro-ajustements uniquement\n",
    "NEW_LR = 1e-5\n",
    "for param_group in model_ppo.policy.optimizer.param_groups:\n",
    "    param_group[\"lr\"] = NEW_LR\n",
    "model_ppo.learning_rate = NEW_LR  # pour cohérence interne\n",
    "\n",
    "# 2) clip_range doit être une FONCTION (schedule), pas un float\n",
    "#    Ici on impose un clip constant à 0.05\n",
    "model_ppo.clip_range = get_schedule_fn(0.05)\n",
    "\n",
    "# 3) Réduire un peu l’exploration intrinsèque\n",
    "model_ppo.ent_coef = 0.001\n",
    "\n",
    "# On laisse n_steps et batch_size tels qu'ils ont été sauvés dans le modèle\n",
    "print(\"\\n=== Hyperparamètres PPO SAFE configurés ===\")\n",
    "print(\"learning_rate      =\", NEW_LR)\n",
    "print(\"clip_range         = constant schedule à 0.05\")\n",
    "print(\"ent_coef           =\", model_ppo.ent_coef)\n",
    "print(\"n_steps (conservé) =\", model_ppo.n_steps)\n",
    "print(\"batch_size (conservé) =\", model_ppo.batch_size)\n",
    "print(\"================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8613758-8b90-4007-a9e7-a6692df5f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 4 — Fonction d'évaluation sur N semaines complètes\n",
    "\n",
    "def evaluate_model_weekly(model, n_episodes: int = 5, max_steps: int = 10080):\n",
    "    \"\"\"\n",
    "    Évalue le modèle sur n_episodes semaines complètes.\n",
    "    Retourne la liste des rewards.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "\n",
    "    for ep in range(n_episodes):\n",
    "        env_eval = WorkshopEnv()\n",
    "        obs, info = env_eval.reset()\n",
    "        total_reward = 0.0\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            mask = env_eval.get_action_mask()\n",
    "            action, _ = model.predict(\n",
    "                obs,\n",
    "                deterministic=True,\n",
    "                action_masks=mask\n",
    "            )\n",
    "            obs, r, terminated, truncated, info = env_eval.step(action)\n",
    "            total_reward += r\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        print(f\"Episode {ep+1}/{n_episodes} — reward = {total_reward:.2f}\")\n",
    "\n",
    "    rewards = np.array(rewards, dtype=np.float32)\n",
    "    print(f\"\\nReward moyen sur {n_episodes} semaines : {rewards.mean():.2f} ± {rewards.std():.2f}\")\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa3e6376-c76f-40f4-9039-5c9bc4a0439e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Évaluation AVANT PPO (policy issue de DAgger) ===\n",
      "Episode 1/5 — reward = 11012.98\n",
      "Episode 2/5 — reward = 10339.32\n",
      "Episode 3/5 — reward = 11129.16\n",
      "Episode 4/5 — reward = 10696.86\n",
      "Episode 5/5 — reward = 9855.36\n",
      "\n",
      "Reward moyen sur 5 semaines : 10606.74 ± 465.22\n"
     ]
    }
   ],
   "source": [
    "# Cellule 5 — Baseline : évaluation AVANT finetuning PPO\n",
    "\n",
    "print(\"=== Évaluation AVANT PPO (policy issue de DAgger) ===\")\n",
    "rewards_before = evaluate_model_weekly(model_ppo, n_episodes=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a165fa3e-d49e-42e0-a76f-948434e73c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callback d'évaluation initialisé.\n"
     ]
    }
   ],
   "source": [
    "# Cellule 6 — Environnement d'évaluation + callback\n",
    "\n",
    "eval_env = ActionMasker(WorkshopEnv(), mask_fn)\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    n_eval_episodes=3,              # 3 semaines complètes pour une éval\n",
    "    eval_freq=20_000,               # tous les 20k timesteps\n",
    "    best_model_save_path=\"./ppo_safe_best\",\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "print(\"Callback d'évaluation initialisé.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a27d77b-d8ce-4272-9630-acefea130b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Entraînement PPO (safe) pour 150000 timesteps ===\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./tb_dagger_hybrid\\ppo_safe_finetune_v1_4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">C:\\Users\\fabri\\anaconda3\\envs\\qlearning\\lib\\site-packages\\rich\\live.py:256: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "C:\\Users\\fabri\\anaconda3\\envs\\qlearning\\lib\\site-packages\\rich\\live.py:256: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 486  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 8    |\n",
      "|    total_timesteps | 4096 |\n",
      "-----------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 400           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 20            |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00029079805 |\n",
      "|    clip_fraction        | 0.0649        |\n",
      "|    clip_range           | 0.05          |\n",
      "|    entropy_loss         | -0.474        |\n",
      "|    explained_variance   | 0.00217       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 413           |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.00277      |\n",
      "|    value_loss           | 857           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.01e+04      |\n",
      "|    ep_rew_mean          | 1.41e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 374           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 32            |\n",
      "|    total_timesteps      | 12288         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00022610225 |\n",
      "|    clip_fraction        | 0.0384        |\n",
      "|    clip_range           | 0.05          |\n",
      "|    entropy_loss         | -0.448        |\n",
      "|    explained_variance   | -0.000999     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 265           |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00158      |\n",
      "|    value_loss           | 597           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.01e+04      |\n",
      "|    ep_rew_mean          | 1.41e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 376           |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 43            |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00035214244 |\n",
      "|    clip_fraction        | 0.0359        |\n",
      "|    clip_range           | 0.05          |\n",
      "|    entropy_loss         | -0.499        |\n",
      "|    explained_variance   | 0.0167        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 326           |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.00256      |\n",
      "|    value_loss           | 723           |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">C:\\Users\\fabri\\anaconda3\\envs\\qlearning\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:70: UserWarning: \n",
       "Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode \n",
       "lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor``\n",
       "wrapper.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "C:\\Users\\fabri\\anaconda3\\envs\\qlearning\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:70: UserWarning: \n",
       "Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode \n",
       "lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor``\n",
       "wrapper.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=20000, episode_reward=3141.21 +/- 128.99\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=20000, episode_reward=3141.21 +/- 128.99\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 10080.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 10080.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.01e+04      |\n",
      "|    mean_reward          | 3.14e+03      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 20000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00053530774 |\n",
      "|    clip_fraction        | 0.0452        |\n",
      "|    clip_range           | 0.05          |\n",
      "|    entropy_loss         | -0.439        |\n",
      "|    explained_variance   | -7.31e-05     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 268           |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.0021       |\n",
      "|    value_loss           | 600           |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "New best mean reward!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.01e+04 |\n",
      "|    ep_rew_mean     | 1.4e+04  |\n",
      "| time/              |          |\n",
      "|    fps             | 225      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 90       |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.01e+04      |\n",
      "|    ep_rew_mean          | 1.4e+04       |\n",
      "| time/                   |               |\n",
      "|    fps                  | 238           |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 103           |\n",
      "|    total_timesteps      | 24576         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00027974125 |\n",
      "|    clip_fraction        | 0.0545        |\n",
      "|    clip_range           | 0.05          |\n",
      "|    entropy_loss         | -0.465        |\n",
      "|    explained_variance   | 0.0181        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 295           |\n",
      "|    n_updates            | 50            |\n",
      "|    policy_gradient_loss | -0.00302      |\n",
      "|    value_loss           | 688           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.01e+04      |\n",
      "|    ep_rew_mean          | 1.4e+04       |\n",
      "| time/                   |               |\n",
      "|    fps                  | 248           |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 115           |\n",
      "|    total_timesteps      | 28672         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021286501 |\n",
      "|    clip_fraction        | 0.0492        |\n",
      "|    clip_range           | 0.05          |\n",
      "|    entropy_loss         | -0.468        |\n",
      "|    explained_variance   | -0.000837     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 418           |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | -0.00175      |\n",
      "|    value_loss           | 810           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.01e+04      |\n",
      "|    ep_rew_mean          | 1.44e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 257           |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 127           |\n",
      "|    total_timesteps      | 32768         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017175374 |\n",
      "|    clip_fraction        | 0.039         |\n",
      "|    clip_range           | 0.05          |\n",
      "|    entropy_loss         | -0.399        |\n",
      "|    explained_variance   | -0.00015      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 281           |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.00162      |\n",
      "|    value_loss           | 662           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.01e+04      |\n",
      "|    ep_rew_mean          | 1.44e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 265           |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 139           |\n",
      "|    total_timesteps      | 36864         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021709048 |\n",
      "|    clip_fraction        | 0.0347        |\n",
      "|    clip_range           | 0.05          |\n",
      "|    entropy_loss         | -0.452        |\n",
      "|    explained_variance   | 0.0113        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 333           |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -0.00215      |\n",
      "|    value_loss           | 750           |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=40000, episode_reward=2865.75 +/- 1057.21\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=40000, episode_reward=2865.75 +/- 1057.21\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 10080.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 10080.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.01e+04      |\n",
      "|    mean_reward          | 2.87e+03      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 40000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019705904 |\n",
      "|    clip_fraction        | 0.0406        |\n",
      "|    clip_range           | 0.05          |\n",
      "|    entropy_loss         | -0.384        |\n",
      "|    explained_variance   | 5.77e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 329           |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | -0.00153      |\n",
      "|    value_loss           | 716           |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.01e+04 |\n",
      "|    ep_rew_mean     | 1.46e+04 |\n",
      "| time/              |          |\n",
      "|    fps             | 224      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 182      |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.01e+04     |\n",
      "|    ep_rew_mean          | 1.46e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 230          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 195          |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002703743 |\n",
      "|    clip_fraction        | 0.0487       |\n",
      "|    clip_range           | 0.05         |\n",
      "|    entropy_loss         | -0.44        |\n",
      "|    explained_variance   | 0.00554      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 330          |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00277     |\n",
      "|    value_loss           | 713          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.01e+04      |\n",
      "|    ep_rew_mean          | 1.46e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 235           |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 208           |\n",
      "|    total_timesteps      | 49152         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00025298374 |\n",
      "|    clip_fraction        | 0.0603        |\n",
      "|    clip_range           | 0.05          |\n",
      "|    entropy_loss         | -0.378        |\n",
      "|    explained_variance   | -0.000432     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 310           |\n",
      "|    n_updates            | 110           |\n",
      "|    policy_gradient_loss | -0.00213      |\n",
      "|    value_loss           | 635           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.01e+04     |\n",
      "|    ep_rew_mean          | 1.49e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 241          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 220          |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001904764 |\n",
      "|    clip_fraction        | 0.0412       |\n",
      "|    clip_range           | 0.05         |\n",
      "|    entropy_loss         | -0.352       |\n",
      "|    explained_variance   | -0.000274    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 324          |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00144     |\n",
      "|    value_loss           | 706          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.01e+04     |\n",
      "|    ep_rew_mean          | 1.49e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 232          |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002494852 |\n",
      "|    clip_fraction        | 0.0626       |\n",
      "|    clip_range           | 0.05         |\n",
      "|    entropy_loss         | -0.399       |\n",
      "|    explained_variance   | 0.00771      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 401          |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00256     |\n",
      "|    value_loss           | 871          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=60000, episode_reward=921.92 +/- 1064.85\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=60000, episode_reward=921.92 +/- 1064.85\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 10080.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 10080.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.01e+04      |\n",
      "|    mean_reward          | 922           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 60000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016708934 |\n",
      "|    clip_fraction        | 0.0492        |\n",
      "|    clip_range           | 0.05          |\n",
      "|    entropy_loss         | -0.381        |\n",
      "|    explained_variance   | 0.000143      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 319           |\n",
      "|    n_updates            | 140           |\n",
      "|    policy_gradient_loss | -0.0016       |\n",
      "|    value_loss           | 672           |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.01e+04 |\n",
      "|    ep_rew_mean     | 1.51e+04 |\n",
      "| time/              |          |\n",
      "|    fps             | 222      |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 275      |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.01e+04      |\n",
      "|    ep_rew_mean          | 1.51e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 227           |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 287           |\n",
      "|    total_timesteps      | 65536         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021730416 |\n",
      "|    clip_fraction        | 0.0411        |\n",
      "|    clip_range           | 0.05          |\n",
      "|    entropy_loss         | -0.389        |\n",
      "|    explained_variance   | 0.0045        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 390           |\n",
      "|    n_updates            | 150           |\n",
      "|    policy_gradient_loss | -0.00189      |\n",
      "|    value_loss           | 760           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.01e+04      |\n",
      "|    ep_rew_mean          | 1.51e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 232           |\n",
      "|    iterations           | 17            |\n",
      "|    time_elapsed         | 299           |\n",
      "|    total_timesteps      | 69632         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013010617 |\n",
      "|    clip_fraction        | 0.0315        |\n",
      "|    clip_range           | 0.05          |\n",
      "|    entropy_loss         | -0.364        |\n",
      "|    explained_variance   | -0.000236     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 321           |\n",
      "|    n_updates            | 160           |\n",
      "|    policy_gradient_loss | -0.00121      |\n",
      "|    value_loss           | 702           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.01e+04      |\n",
      "|    ep_rew_mean          | 1.55e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 236           |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 311           |\n",
      "|    total_timesteps      | 73728         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018784737 |\n",
      "|    clip_fraction        | 0.0495        |\n",
      "|    clip_range           | 0.05          |\n",
      "|    entropy_loss         | -0.323        |\n",
      "|    explained_variance   | -5.21e-05     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 336           |\n",
      "|    n_updates            | 170           |\n",
      "|    policy_gradient_loss | -0.00166      |\n",
      "|    value_loss           | 689           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.01e+04     |\n",
      "|    ep_rew_mean          | 1.55e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 239          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 325          |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002267143 |\n",
      "|    clip_fraction        | 0.0429       |\n",
      "|    clip_range           | 0.05         |\n",
      "|    entropy_loss         | -0.4         |\n",
      "|    explained_variance   | 0.00974      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 426          |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00203     |\n",
      "|    value_loss           | 884          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=80000, episode_reward=3435.71 +/- 350.63\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=80000, episode_reward=3435.71 +/- 350.63\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 10080.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 10080.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.01e+04      |\n",
      "|    mean_reward          | 3.44e+03      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 80000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00024990595 |\n",
      "|    clip_fraction        | 0.0364        |\n",
      "|    clip_range           | 0.05          |\n",
      "|    entropy_loss         | -0.362        |\n",
      "|    explained_variance   | -0.000196     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 357           |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | -0.00149      |\n",
      "|    value_loss           | 762           |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "New best mean reward!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.01e+04 |\n",
      "|    ep_rew_mean     | 1.57e+04 |\n",
      "| time/              |          |\n",
      "|    fps             | 221      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 370      |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.01e+04      |\n",
      "|    ep_rew_mean          | 1.57e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 225           |\n",
      "|    iterations           | 21            |\n",
      "|    time_elapsed         | 381           |\n",
      "|    total_timesteps      | 86016         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019713705 |\n",
      "|    clip_fraction        | 0.033         |\n",
      "|    clip_range           | 0.05          |\n",
      "|    entropy_loss         | -0.398        |\n",
      "|    explained_variance   | 0.00492       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 324           |\n",
      "|    n_updates            | 200           |\n",
      "|    policy_gradient_loss | -0.00178      |\n",
      "|    value_loss           | 741           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.01e+04     |\n",
      "|    ep_rew_mean          | 1.57e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 230          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 390          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001331434 |\n",
      "|    clip_fraction        | 0.0352       |\n",
      "|    clip_range           | 0.05         |\n",
      "|    entropy_loss         | -0.346       |\n",
      "|    explained_variance   | -0.000162    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 368          |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00114     |\n",
      "|    value_loss           | 775          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.01e+04     |\n",
      "|    ep_rew_mean          | 1.6e+04      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 235          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 400          |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.987941e-05 |\n",
      "|    clip_fraction        | 0.0296       |\n",
      "|    clip_range           | 0.05         |\n",
      "|    entropy_loss         | -0.332       |\n",
      "|    explained_variance   | -1.73e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 348          |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00146     |\n",
      "|    value_loss           | 745          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.01e+04      |\n",
      "|    ep_rew_mean          | 1.6e+04       |\n",
      "| time/                   |               |\n",
      "|    fps                  | 239           |\n",
      "|    iterations           | 24            |\n",
      "|    time_elapsed         | 410           |\n",
      "|    total_timesteps      | 98304         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021296719 |\n",
      "|    clip_fraction        | 0.0346        |\n",
      "|    clip_range           | 0.05          |\n",
      "|    entropy_loss         | -0.37         |\n",
      "|    explained_variance   | 0.00949       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 348           |\n",
      "|    n_updates            | 230           |\n",
      "|    policy_gradient_loss | -0.00205      |\n",
      "|    value_loss           | 733           |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=100000, episode_reward=1009.72 +/- 1194.86\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=100000, episode_reward=1009.72 +/- 1194.86\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 10080.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 10080.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.01e+04     |\n",
      "|    mean_reward          | 1.01e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 100000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.019212e-05 |\n",
      "|    clip_fraction        | 0.0267       |\n",
      "|    clip_range           | 0.05         |\n",
      "|    entropy_loss         | -0.297       |\n",
      "|    explained_variance   | -6.83e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 328          |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00109     |\n",
      "|    value_loss           | 680          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.01e+04 |\n",
      "|    ep_rew_mean     | 1.62e+04 |\n",
      "| time/              |          |\n",
      "|    fps             | 227      |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 450      |\n",
      "|    total_timesteps | 102400   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.01e+04      |\n",
      "|    ep_rew_mean          | 1.62e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 230           |\n",
      "|    iterations           | 26            |\n",
      "|    time_elapsed         | 462           |\n",
      "|    total_timesteps      | 106496        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017582119 |\n",
      "|    clip_fraction        | 0.0341        |\n",
      "|    clip_range           | 0.05          |\n",
      "|    entropy_loss         | -0.387        |\n",
      "|    explained_variance   | 0.0128        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 331           |\n",
      "|    n_updates            | 250           |\n",
      "|    policy_gradient_loss | -0.00209      |\n",
      "|    value_loss           | 699           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.01e+04      |\n",
      "|    ep_rew_mean          | 1.62e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 233           |\n",
      "|    iterations           | 27            |\n",
      "|    time_elapsed         | 473           |\n",
      "|    total_timesteps      | 110592        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.6601864e-05 |\n",
      "|    clip_fraction        | 0.0269        |\n",
      "|    clip_range           | 0.05          |\n",
      "|    entropy_loss         | -0.337        |\n",
      "|    explained_variance   | -6.27e-05     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 322           |\n",
      "|    n_updates            | 260           |\n",
      "|    policy_gradient_loss | -0.00138      |\n",
      "|    value_loss           | 640           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.01e+04      |\n",
      "|    ep_rew_mean          | 1.63e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 236           |\n",
      "|    iterations           | 28            |\n",
      "|    time_elapsed         | 484           |\n",
      "|    total_timesteps      | 114688        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017287939 |\n",
      "|    clip_fraction        | 0.0324        |\n",
      "|    clip_range           | 0.05          |\n",
      "|    entropy_loss         | -0.314        |\n",
      "|    explained_variance   | -4.79e-05     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 310           |\n",
      "|    n_updates            | 270           |\n",
      "|    policy_gradient_loss | -0.00118      |\n",
      "|    value_loss           | 660           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.01e+04      |\n",
      "|    ep_rew_mean          | 1.63e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 239           |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 495           |\n",
      "|    total_timesteps      | 118784        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018489372 |\n",
      "|    clip_fraction        | 0.0358        |\n",
      "|    clip_range           | 0.05          |\n",
      "|    entropy_loss         | -0.355        |\n",
      "|    explained_variance   | 0.0246        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 323           |\n",
      "|    n_updates            | 280           |\n",
      "|    policy_gradient_loss | -0.00214      |\n",
      "|    value_loss           | 635           |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=120000, episode_reward=2539.58 +/- 287.54\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=120000, episode_reward=2539.58 +/- 287.54\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 10080.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 10080.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.01e+04     |\n",
      "|    mean_reward          | 2.54e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 120000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.862999e-05 |\n",
      "|    clip_fraction        | 0.0222       |\n",
      "|    clip_range           | 0.05         |\n",
      "|    entropy_loss         | -0.31        |\n",
      "|    explained_variance   | 1.88e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 304          |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00108     |\n",
      "|    value_loss           | 650          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.01e+04 |\n",
      "|    ep_rew_mean     | 1.64e+04 |\n",
      "| time/              |          |\n",
      "|    fps             | 228      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 536      |\n",
      "|    total_timesteps | 122880   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.01e+04      |\n",
      "|    ep_rew_mean          | 1.64e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 232           |\n",
      "|    iterations           | 31            |\n",
      "|    time_elapsed         | 547           |\n",
      "|    total_timesteps      | 126976        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014709313 |\n",
      "|    clip_fraction        | 0.0304        |\n",
      "|    clip_range           | 0.05          |\n",
      "|    entropy_loss         | -0.406        |\n",
      "|    explained_variance   | 0.0903        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 296           |\n",
      "|    n_updates            | 300           |\n",
      "|    policy_gradient_loss | -0.00178      |\n",
      "|    value_loss           | 602           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.01e+04     |\n",
      "|    ep_rew_mean          | 1.65e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 234          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 558          |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001411732 |\n",
      "|    clip_fraction        | 0.0354       |\n",
      "|    clip_range           | 0.05         |\n",
      "|    entropy_loss         | -0.372       |\n",
      "|    explained_variance   | -0.000324    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 256          |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00143     |\n",
      "|    value_loss           | 598          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.01e+04      |\n",
      "|    ep_rew_mean          | 1.65e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 237           |\n",
      "|    iterations           | 33            |\n",
      "|    time_elapsed         | 570           |\n",
      "|    total_timesteps      | 135168        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012279293 |\n",
      "|    clip_fraction        | 0.0346        |\n",
      "|    clip_range           | 0.05          |\n",
      "|    entropy_loss         | -0.351        |\n",
      "|    explained_variance   | 0.139         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 303           |\n",
      "|    n_updates            | 320           |\n",
      "|    policy_gradient_loss | -0.00176      |\n",
      "|    value_loss           | 656           |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 1.01e+04       |\n",
      "|    ep_rew_mean          | 1.65e+04       |\n",
      "| time/                   |                |\n",
      "|    fps                  | 239            |\n",
      "|    iterations           | 34             |\n",
      "|    time_elapsed         | 581            |\n",
      "|    total_timesteps      | 139264         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000121511795 |\n",
      "|    clip_fraction        | 0.0227         |\n",
      "|    clip_range           | 0.05           |\n",
      "|    entropy_loss         | -0.331         |\n",
      "|    explained_variance   | 0.293          |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 284            |\n",
      "|    n_updates            | 330            |\n",
      "|    policy_gradient_loss | -0.00143       |\n",
      "|    value_loss           | 640            |\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=140000, episode_reward=-1948.31 +/- 864.73\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=140000, episode_reward=-1948.31 +/- 864.73\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 10080.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 10080.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1.01e+04      |\n",
      "|    mean_reward          | -1.95e+03     |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 140000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017972986 |\n",
      "|    clip_fraction        | 0.0349        |\n",
      "|    clip_range           | 0.05          |\n",
      "|    entropy_loss         | -0.362        |\n",
      "|    explained_variance   | 7.37e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 295           |\n",
      "|    n_updates            | 340           |\n",
      "|    policy_gradient_loss | -0.00146      |\n",
      "|    value_loss           | 580           |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.01e+04 |\n",
      "|    ep_rew_mean     | 1.65e+04 |\n",
      "| time/              |          |\n",
      "|    fps             | 231      |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 619      |\n",
      "|    total_timesteps | 143360   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.01e+04      |\n",
      "|    ep_rew_mean          | 1.65e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 233           |\n",
      "|    iterations           | 36            |\n",
      "|    time_elapsed         | 630           |\n",
      "|    total_timesteps      | 147456        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00022829196 |\n",
      "|    clip_fraction        | 0.0472        |\n",
      "|    clip_range           | 0.05          |\n",
      "|    entropy_loss         | -0.503        |\n",
      "|    explained_variance   | 0.353         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 258           |\n",
      "|    n_updates            | 350           |\n",
      "|    policy_gradient_loss | -0.00283      |\n",
      "|    value_loss           | 580           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.01e+04      |\n",
      "|    ep_rew_mean          | 1.65e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 236           |\n",
      "|    iterations           | 37            |\n",
      "|    time_elapsed         | 640           |\n",
      "|    total_timesteps      | 151552        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013301721 |\n",
      "|    clip_fraction        | 0.0395        |\n",
      "|    clip_range           | 0.05          |\n",
      "|    entropy_loss         | -0.407        |\n",
      "|    explained_variance   | 2.74e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 230           |\n",
      "|    n_updates            | 360           |\n",
      "|    policy_gradient_loss | -0.00148      |\n",
      "|    value_loss           | 483           |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fin de l'entraînement PPO (safe) ===\n"
     ]
    }
   ],
   "source": [
    "# Cellule 7 — Entraînement PPO avec contraintes \"safe\"\n",
    "\n",
    "TOTAL_TIMESTEPS = 150_000\n",
    "LOG_NAME = \"ppo_safe_finetune_v1\"\n",
    "\n",
    "print(f\"=== Entraînement PPO (safe) pour {TOTAL_TIMESTEPS} timesteps ===\")\n",
    "\n",
    "model_ppo.set_env(env_train)  # par sécurité\n",
    "\n",
    "model_ppo.learn(\n",
    "    total_timesteps=TOTAL_TIMESTEPS,\n",
    "    tb_log_name=LOG_NAME,\n",
    "    callback=eval_callback,\n",
    "    progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"=== Fin de l'entraînement PPO (safe) ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35cb71ed-6bc1-420c-bd7d-4ba8e0129f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model chargé depuis : ./ppo_safe_best/best_model.zip\n",
      "\n",
      "=== Évaluation APRÈS PPO (policy finetunée) ===\n",
      "Episode 1/5 — reward = 13045.70\n",
      "Episode 2/5 — reward = 12649.44\n",
      "Episode 3/5 — reward = 12940.06\n",
      "Episode 4/5 — reward = 13098.62\n",
      "Episode 5/5 — reward = 12289.66\n",
      "\n",
      "Reward moyen sur 5 semaines : 12804.70 ± 300.72\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# Cellule 8 — Évaluation du modèle PPO finetuné (CORRIGÉE)\n",
    "# ======================================================\n",
    "\n",
    "def evaluate_agent(model, n_episodes=5, max_steps=10080):\n",
    "    rewards = []\n",
    "\n",
    "    for ep in range(n_episodes):\n",
    "        env = WorkshopEnv()            # environnement brut\n",
    "        obs, info = env.reset()\n",
    "        total_r = 0.0\n",
    "\n",
    "        for t in range(max_steps):\n",
    "\n",
    "            mask = env.get_action_mask()\n",
    "\n",
    "            # PREDICTION CORRECTE POUR MASKABLEPPO\n",
    "            action, _ = model.predict(\n",
    "                obs,                    # simple vecteur\n",
    "                deterministic=True,\n",
    "                action_masks=mask       # masque passé séparément\n",
    "            )\n",
    "\n",
    "            obs, r, terminated, truncated, info = env.step(action)\n",
    "            total_r += r\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        rewards.append(total_r)\n",
    "        print(f\"Episode {ep+1}/{n_episodes} — reward = {total_r:.2f}\")\n",
    "\n",
    "    rewards = np.array(rewards, dtype=np.float32)\n",
    "    print(f\"\\nReward moyen sur {n_episodes} semaines : \"\n",
    "          f\"{rewards.mean():.2f} ± {rewards.std():.2f}\")\n",
    "\n",
    "    return rewards, rewards.mean(), rewards.std()\n",
    "\n",
    "\n",
    "# ========= CHARGEMENT DU BEST MODEL ==========\n",
    "BEST_MODEL_PATH = \"./ppo_safe_best/best_model.zip\"\n",
    "\n",
    "try:\n",
    "    best_model = MaskablePPO.load(BEST_MODEL_PATH, device=\"cpu\")\n",
    "    print(f\"\\nBest model chargé depuis : {BEST_MODEL_PATH}\")\n",
    "except Exception as e:\n",
    "    print(\"\\n⚠ Impossible de charger le best model, utilisation du model_ppo final.\")\n",
    "    best_model = model_ppo\n",
    "\n",
    "\n",
    "# ========= ÉVALUATION ==========\n",
    "print(\"\\n=== Évaluation APRÈS PPO (policy finetunée) ===\")\n",
    "rewards_after, mean_after, std_after = evaluate_agent(best_model, n_episodes=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f17a4977-52e5-4cbc-8071-c74294e236ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Modèle PPO finetuné sauvegardé avec succès !\n",
      "Chemin : ppo_final_model\\ppo_finetuned_20251211_1504.zip\n",
      "Hyperparamètres enregistrés dans : ppo_final_model/hyperparams.txt\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# cellule 9 : Sauvegarde propre du modèle PPO finetuné\n",
    "# ======================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Dossier de sauvegarde\n",
    "SAVE_DIR = \"ppo_final_model\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Nom du fichier = date + heure pour versioning automatique\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "MODEL_PATH = os.path.join(SAVE_DIR, f\"ppo_finetuned_{timestamp}.zip\")\n",
    "\n",
    "# 1) Sauvegarde du modèle complet\n",
    "best_model.save(MODEL_PATH)\n",
    "\n",
    "# 2) Sauvegarde des hyperparamètres dans un fichier texte\n",
    "with open(os.path.join(SAVE_DIR, \"hyperparams.txt\"), \"w\") as f:\n",
    "    f.write(\"=== PPO Finetuned Hyperparameters ===\\n\")\n",
    "    f.write(f\"learning_rate = {best_model.learning_rate}\\n\")\n",
    "    f.write(f\"clip_range    = constant schedule 0.05\\n\")\n",
    "    f.write(f\"ent_coef      = {best_model.ent_coef}\\n\")\n",
    "    f.write(f\"n_steps       = {best_model.n_steps}\\n\")\n",
    "    f.write(f\"batch_size    = {best_model.batch_size}\\n\")\n",
    "    f.write(f\"timestamp     = {timestamp}\\n\")\n",
    "\n",
    "# 3) Message confirmation\n",
    "print(\"======================================\")\n",
    "print(\"Modèle PPO finetuné sauvegardé avec succès !\")\n",
    "print(\"Chemin :\", MODEL_PATH)\n",
    "print(\"Hyperparamètres enregistrés dans : ppo_final_model/hyperparams.txt\")\n",
    "print(\"======================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6136abda-17bf-4a2f-9dce-a74e66476c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:qlearning]",
   "language": "python",
   "name": "conda-env-qlearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
