{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e752a35-c9d5-492d-81ea-19f5c15f7352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x28f1e660cd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cellule 1 — Imports et configuration de base\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "import json\n",
    "import time\n",
    "\n",
    "from gymnasium import spaces\n",
    "\n",
    "from sb3_contrib import MaskablePPO\n",
    "from env.workshop_env import WorkshopEnv\n",
    "\n",
    "# === Historique des métriques DAgger ===\n",
    "history = {\n",
    "    \"supervised_loss\": [],   # perte moyenne par itération DAgger\n",
    "    \"reward_collect\": [],    # reward élève pendant la phase de collecte\n",
    "    \"reward_eval\": []        # reward élève en évaluation (7 jours)\n",
    "}\n",
    "\n",
    "\n",
    "# Pour rendre les choses un peu reproductibles\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b89e3090-cdb1-448a-8bf7-70426116a22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 2 — Politique experte v3 compatible obs=23 variables + normalisation\n",
    "\n",
    "from env.workshop_env import WorkshopEnv\n",
    "import numpy as np\n",
    "\n",
    "def expert_policy(obs: np.ndarray, env: WorkshopEnv) -> int:\n",
    "    \"\"\"\n",
    "    Politique experte v3 améliorée et sécurisée :\n",
    "    - réagit à l'ampleur des backlogs\n",
    "    - commande les MP proportionnellement au backlog total (sécurisé)\n",
    "    - choisit k en fonction de l'urgence (max 5)\n",
    "    - priorise P2 puis P1\n",
    "    - termine d'abord les P2_inter\n",
    "    - ne renvoie jamais une action hors [0, 200] si on l'utilise correctement via expert_policy_masked\n",
    "    \"\"\"\n",
    "\n",
    "    # ============================================================\n",
    "    # 1) Dé-normalisation des variables d'état\n",
    "    # ============================================================\n",
    "\n",
    "    time = float(obs[0]) * float(env.max_time)\n",
    "\n",
    "    m1_busy = int(round(obs[1]))\n",
    "    m1_time_left = float(obs[2]) * 100.0\n",
    "\n",
    "    m2_busy = int(round(obs[3]))\n",
    "    m2_time_left = float(obs[4]) * 100.0\n",
    "\n",
    "    stock_raw = float(obs[5]) * float(env.raw_capacity)\n",
    "    stock_p1 = float(obs[6]) * float(env.raw_capacity)\n",
    "    stock_p2_inter = float(obs[7]) * float(env.raw_capacity)\n",
    "    stock_p2 = float(obs[8]) * float(env.raw_capacity)\n",
    "\n",
    "    next_delivery_cd = float(obs[9]) * 10080.0\n",
    "\n",
    "    demande_p1 = float(obs[10]) * 1000.0\n",
    "    demande_p2 = float(obs[11]) * 1000.0\n",
    "    q_raw_incoming = float(obs[12]) * 1000.0\n",
    "\n",
    "    m1_free = (m1_busy == 0)\n",
    "    m2_free = (m2_busy == 0)\n",
    "\n",
    "    backlog_p1 = max(demande_p1, 0.0)\n",
    "    backlog_p2 = max(demande_p2, 0.0)\n",
    "    backlog_total = backlog_p1 + backlog_p2\n",
    "\n",
    "    # ============================================================\n",
    "    # 2) Heuristique dynamique pour choisir k selon backlog\n",
    "    # ============================================================\n",
    "\n",
    "    def choose_k(backlog, k_max=5):\n",
    "        \"\"\"\n",
    "        Retourne un entier k :\n",
    "        - 0 si k_max < 1 (pas de prod possible)\n",
    "        - sinon entre 1 et k_max_int\n",
    "        \"\"\"\n",
    "        k_max_int = int(k_max)\n",
    "        if k_max_int < 1:\n",
    "            return 0\n",
    "\n",
    "        if backlog <= 5:       k = 1\n",
    "        elif backlog <= 15:    k = 2\n",
    "        elif backlog <= 30:    k = 3\n",
    "        elif backlog <= 60:    k = 4\n",
    "        else:                  k = 5\n",
    "\n",
    "        k = min(k, k_max_int)\n",
    "        return max(1, int(k))\n",
    "\n",
    "    # ============================================================\n",
    "    # 3) Politique de commande MP (version sécurisée)\n",
    "    # ============================================================\n",
    "\n",
    "    target_raw = min(env.raw_capacity, backlog_total + 10.0)\n",
    "    current_pipeline = max(0.0, stock_raw + q_raw_incoming)\n",
    "\n",
    "    missing = target_raw - current_pipeline\n",
    "\n",
    "    if missing > 0:\n",
    "        # BORNE STRICTE : 1 ≤ k_cmd ≤ 50\n",
    "        k_cmd = int(max(1, min(50, missing)))\n",
    "        action_cmd = 149 + k_cmd  # 150 → 199\n",
    "        # Sécurité supplémentaire\n",
    "        if 0 <= action_cmd <= 200:\n",
    "            return action_cmd\n",
    "\n",
    "    # ============================================================\n",
    "    # 4) Priorité absolue : terminer STEP2_P2 si M2 libre\n",
    "    # ============================================================\n",
    "\n",
    "    if m2_free and stock_p2_inter > 0:\n",
    "        k2 = choose_k(backlog_p2, k_max=min(5, stock_p2_inter))\n",
    "        if k2 > 0:\n",
    "            action_p2_step2 = 99 + k2  # 100 → 149\n",
    "            if 0 <= action_p2_step2 <= 200:\n",
    "                return action_p2_step2\n",
    "\n",
    "    # ============================================================\n",
    "    # 5) Priorité n°2 : produire STEP1_P2 si backlog P2 > 0\n",
    "    # ============================================================\n",
    "\n",
    "    if m1_free and backlog_p2 > 0 and stock_raw > 0:\n",
    "        k1_p2 = choose_k(backlog_p2, k_max=min(5, stock_raw))\n",
    "        if k1_p2 > 0:\n",
    "            action_p2_step1 = 49 + k1_p2  # 50 → 99\n",
    "            if 0 <= action_p2_step1 <= 200:\n",
    "                return action_p2_step1\n",
    "\n",
    "    # ============================================================\n",
    "    # 6) Sinon, produire P1 si backlog P1 > 0\n",
    "    # ============================================================\n",
    "\n",
    "    if m1_free and backlog_p1 > 0 and stock_raw > 0:\n",
    "        k1_p1 = choose_k(backlog_p1, k_max=min(5, stock_raw))\n",
    "        if k1_p1 > 0:\n",
    "            action_p1 = k1_p1 - 1         # 0 → 49\n",
    "            if 0 <= action_p1 <= 200:\n",
    "                return action_p1\n",
    "\n",
    "    # ============================================================\n",
    "    # 7) Sinon WAIT\n",
    "    # ============================================================\n",
    "\n",
    "    return 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61926e71-846c-46d2-9411-91a017edb72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 3 — Politique experte masquée + fonction d'épisode expert\n",
    "\n",
    "def expert_policy_masked(env: WorkshopEnv, obs: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Politique experte avec prise en compte du masque d'actions.\n",
    "    Sécurisée : ne renvoie jamais une action hors [0, 200].\n",
    "    \"\"\"\n",
    "    mask = env.get_action_mask().astype(bool)\n",
    "\n",
    "    a = expert_policy(obs, env)  # action brute proposée par l'expert\n",
    "\n",
    "    # Sécurité sur le type et la plage des indices\n",
    "    if not isinstance(a, (int, np.integer)):\n",
    "        a = 200\n",
    "    elif a < 0 or a >= len(mask):\n",
    "        a = 200\n",
    "\n",
    "    # Si l'action est invalide selon le masque, on corrige\n",
    "    if not mask[a]:\n",
    "        # Priorité : WAIT si autorisé\n",
    "        if mask[200]:\n",
    "            return 200\n",
    "        # Sinon, on prend la première action valide\n",
    "        valid_actions = np.where(mask)[0]\n",
    "        return int(valid_actions[0])\n",
    "\n",
    "    return int(a)\n",
    "\n",
    "\n",
    "def run_expert_episode(env: WorkshopEnv, max_steps: int = 10080):\n",
    "    \"\"\"\n",
    "    Joue un épisode complet avec l'expert masqué.\n",
    "    Renvoie :\n",
    "      - obs_array : (T, 23)  # 23 features normalisées\n",
    "      - act_array : (T,)\n",
    "      - total_reward\n",
    "      - nb_steps\n",
    "    \"\"\"\n",
    "    obs, info = env.reset()\n",
    "    obs_list = []\n",
    "    act_list = []\n",
    "    total_reward = 0.0\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        action = expert_policy_masked(env, obs)\n",
    "        obs_list.append(obs.copy())\n",
    "        act_list.append(action)\n",
    "\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    obs_array = np.stack(obs_list, axis=0)\n",
    "    act_array = np.array(act_list, dtype=np.int64)\n",
    "\n",
    "    return obs_array, act_array, total_reward, t + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "add720d5-bc1a-4847-8baa-4cbe2055e1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Génération du dataset initial expert ===\n",
      "\n",
      "--- Episode expert 1/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12530.12\n",
      "\n",
      "--- Episode expert 2/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12938.30\n",
      "\n",
      "--- Episode expert 3/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12658.06\n",
      "\n",
      "--- Episode expert 4/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13957.10\n",
      "\n",
      "--- Episode expert 5/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13475.06\n",
      "\n",
      "--- Episode expert 6/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13042.08\n",
      "\n",
      "--- Episode expert 7/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12440.06\n",
      "\n",
      "--- Episode expert 8/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12760.18\n",
      "\n",
      "--- Episode expert 9/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13147.98\n",
      "\n",
      "--- Episode expert 10/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13139.46\n",
      "\n",
      "--- Episode expert 11/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13264.54\n",
      "\n",
      "--- Episode expert 12/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 14069.56\n",
      "\n",
      "--- Episode expert 13/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 14221.52\n",
      "\n",
      "--- Episode expert 14/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13448.26\n",
      "\n",
      "--- Episode expert 15/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13124.98\n",
      "\n",
      "--- Episode expert 16/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13169.84\n",
      "\n",
      "--- Episode expert 17/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13233.80\n",
      "\n",
      "--- Episode expert 18/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12448.98\n",
      "\n",
      "--- Episode expert 19/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13505.26\n",
      "\n",
      "--- Episode expert 20/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13192.58\n",
      "\n",
      "--- Episode expert 21/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12878.94\n",
      "\n",
      "--- Episode expert 22/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12945.06\n",
      "\n",
      "--- Episode expert 23/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12813.04\n",
      "\n",
      "--- Episode expert 24/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13076.44\n",
      "\n",
      "--- Episode expert 25/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13526.32\n",
      "\n",
      "--- Episode expert 26/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12878.74\n",
      "\n",
      "--- Episode expert 27/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12858.26\n",
      "\n",
      "--- Episode expert 28/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12170.80\n",
      "\n",
      "--- Episode expert 29/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12850.28\n",
      "\n",
      "--- Episode expert 30/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13940.48\n",
      "\n",
      "--- Episode expert 31/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13129.66\n",
      "\n",
      "--- Episode expert 32/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13750.14\n",
      "\n",
      "--- Episode expert 33/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13434.16\n",
      "\n",
      "--- Episode expert 34/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 14060.70\n",
      "\n",
      "--- Episode expert 35/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13606.68\n",
      "\n",
      "--- Episode expert 36/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12877.00\n",
      "\n",
      "--- Episode expert 37/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13098.76\n",
      "\n",
      "--- Episode expert 38/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13303.02\n",
      "\n",
      "--- Episode expert 39/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13276.38\n",
      "\n",
      "--- Episode expert 40/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13662.78\n",
      "\n",
      "--- Episode expert 41/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13970.76\n",
      "\n",
      "--- Episode expert 42/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13925.20\n",
      "\n",
      "--- Episode expert 43/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12946.42\n",
      "\n",
      "--- Episode expert 44/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12499.04\n",
      "\n",
      "--- Episode expert 45/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12921.94\n",
      "\n",
      "--- Episode expert 46/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13221.06\n",
      "\n",
      "--- Episode expert 47/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13568.08\n",
      "\n",
      "--- Episode expert 48/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12907.46\n",
      "\n",
      "--- Episode expert 49/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13253.40\n",
      "\n",
      "--- Episode expert 50/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13784.06\n",
      "\n",
      "--- Episode expert 51/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13482.70\n",
      "\n",
      "--- Episode expert 52/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13011.92\n",
      "\n",
      "--- Episode expert 53/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13533.94\n",
      "\n",
      "--- Episode expert 54/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12526.74\n",
      "\n",
      "--- Episode expert 55/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13138.00\n",
      "\n",
      "--- Episode expert 56/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 11917.10\n",
      "\n",
      "--- Episode expert 57/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13364.88\n",
      "\n",
      "--- Episode expert 58/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13686.88\n",
      "\n",
      "--- Episode expert 59/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12960.04\n",
      "\n",
      "--- Episode expert 60/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12901.34\n",
      "\n",
      "--- Episode expert 61/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12999.58\n",
      "\n",
      "--- Episode expert 62/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13440.42\n",
      "\n",
      "--- Episode expert 63/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13191.26\n",
      "\n",
      "--- Episode expert 64/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13175.42\n",
      "\n",
      "--- Episode expert 65/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12840.02\n",
      "\n",
      "--- Episode expert 66/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13073.92\n",
      "\n",
      "--- Episode expert 67/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12721.74\n",
      "\n",
      "--- Episode expert 68/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12776.22\n",
      "\n",
      "--- Episode expert 69/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13194.24\n",
      "\n",
      "--- Episode expert 70/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13553.70\n",
      "\n",
      "--- Episode expert 71/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12812.74\n",
      "\n",
      "--- Episode expert 72/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13218.86\n",
      "\n",
      "--- Episode expert 73/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13227.08\n",
      "\n",
      "--- Episode expert 74/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13746.34\n",
      "\n",
      "--- Episode expert 75/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13537.20\n",
      "\n",
      "--- Episode expert 76/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13740.44\n",
      "\n",
      "--- Episode expert 77/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12915.08\n",
      "\n",
      "--- Episode expert 78/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13589.32\n",
      "\n",
      "--- Episode expert 79/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12505.12\n",
      "\n",
      "--- Episode expert 80/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12775.68\n",
      "\n",
      "--- Episode expert 81/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13469.76\n",
      "\n",
      "--- Episode expert 82/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12182.16\n",
      "\n",
      "--- Episode expert 83/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13393.48\n",
      "\n",
      "--- Episode expert 84/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13218.76\n",
      "\n",
      "--- Episode expert 85/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13470.78\n",
      "\n",
      "--- Episode expert 86/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13758.08\n",
      "\n",
      "--- Episode expert 87/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13259.72\n",
      "\n",
      "--- Episode expert 88/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13718.36\n",
      "\n",
      "--- Episode expert 89/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12981.40\n",
      "\n",
      "--- Episode expert 90/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13418.62\n",
      "\n",
      "--- Episode expert 91/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13702.74\n",
      "\n",
      "--- Episode expert 92/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13025.02\n",
      "\n",
      "--- Episode expert 93/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 14073.72\n",
      "\n",
      "--- Episode expert 94/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13278.42\n",
      "\n",
      "--- Episode expert 95/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 14136.18\n",
      "\n",
      "--- Episode expert 96/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12954.14\n",
      "\n",
      "--- Episode expert 97/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 14546.12\n",
      "\n",
      "--- Episode expert 98/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13108.20\n",
      "\n",
      "--- Episode expert 99/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 12839.96\n",
      "\n",
      "--- Episode expert 100/100 ---\n",
      "  ↳ Longueur épisode : 10080 steps\n",
      "  ↳ Reward expert : 13232.80\n",
      "\n",
      "=== Dataset expert initial généré ===\n",
      "Taille dagger_obs : (1008000, 23)\n",
      "Taille dagger_actions : (1008000,)\n",
      "Reward moyen expert sur 100 épisodes : 13222.29\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELLULE 4 — Générer un dataset initial de 10 épisodes experts\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "N_EXPERT_EPISODES = 100   # Nombre d'épisodes experts à générer\n",
    "\n",
    "dagger_obs = []\n",
    "dagger_actions = []\n",
    "expert_rewards = []\n",
    "\n",
    "print(\"=== Génération du dataset initial expert ===\")\n",
    "\n",
    "for ep in range(N_EXPERT_EPISODES):\n",
    "    print(f\"\\n--- Episode expert {ep+1}/{N_EXPERT_EPISODES} ---\")\n",
    "    \n",
    "    env_exp = WorkshopEnv()                 # nouvel environnement frais\n",
    "    obs_ep, act_ep, rew_ep, length_ep = run_expert_episode(env_exp)\n",
    "\n",
    "    # Stockage\n",
    "    dagger_obs.append(obs_ep)\n",
    "    dagger_actions.append(act_ep)\n",
    "    expert_rewards.append(rew_ep)\n",
    "\n",
    "    print(f\"  ↳ Longueur épisode : {length_ep} steps\")\n",
    "    print(f\"  ↳ Reward expert : {rew_ep:.2f}\")\n",
    "\n",
    "# Fusion des données des 10 épisodes\n",
    "dagger_obs = np.vstack(dagger_obs)\n",
    "dagger_actions = np.hstack(dagger_actions)\n",
    "\n",
    "print(\"\\n=== Dataset expert initial généré ===\")\n",
    "print(\"Taille dagger_obs :\", dagger_obs.shape)\n",
    "print(\"Taille dagger_actions :\", dagger_actions.shape)\n",
    "print(f\"Reward moyen expert sur 100 épisodes : {np.mean(expert_rewards):.2f}\")\n",
    "print(\"===============================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d984b4c8-9b25-4bd5-8099-f1d6eef49644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Modèle élève initialisé (MaskablePPO + CPU + ActionMasker OK).\n"
     ]
    }
   ],
   "source": [
    "# Cellule 5 — Environnement avec ActionMasker + Modèle MaskablePPO (CPU)\n",
    "\n",
    "import gymnasium as gym\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "\n",
    "# 1) Fonction de masquage (appelée automatiquement par ActionMasker)\n",
    "def mask_fn(env):\n",
    "    return env.get_action_mask()\n",
    "\n",
    "# 2) Environnement enveloppé\n",
    "env_student = ActionMasker(WorkshopEnv(), mask_fn)\n",
    "\n",
    "# 3) Modèle MaskablePPO — FORCÉ SUR CPU\n",
    "from sb3_contrib import MaskablePPO\n",
    "\n",
    "model_student = MaskablePPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env_student,\n",
    "    verbose=1,\n",
    "    device=\"cpu\",              # <<< ICI : CPU, plus \"cuda\"\n",
    "    learning_rate=3e-4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    n_steps=4096,\n",
    "    batch_size=512,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.01,\n",
    "    max_grad_norm=0.5,\n",
    "    tensorboard_log=\"./tb_dagger_hybrid\"\n",
    ")\n",
    "\n",
    "print(\"Modèle élève initialisé (MaskablePPO + CPU + ActionMasker OK).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e88ee590-9215-487f-a99d-d95d5c43a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CELLULE 6 — Entraînement supervisé DAgger (VERSION LÉGÈRE) \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_student_supervised(model, obs_array, act_array, epochs=5, batch_size=256):\n",
    "    \"\"\"\n",
    "    Entraînement supervisé du student :\n",
    "    Il apprend à imiter l'expert sur le buffer DAgger.\n",
    "\n",
    "    obs_array : (N, 23)\n",
    "    act_array : (N,)\n",
    "\n",
    "    VERSION LÉGÈRE :\n",
    "    - on limite le nombre d'exemples utilisés à MAX_TRAIN_SAMPLES\n",
    "      pour éviter de saturer le CPU / la RAM.\n",
    "    \"\"\"\n",
    "\n",
    "    # 0) Limiter la taille du dataset pour rester \"safe\"\n",
    "    MAX_TRAIN_SAMPLES = 12000  # à ajuster, mais déjà bien plus léger que 30000\n",
    "\n",
    "    N_total = len(obs_array)\n",
    "    if N_total > MAX_TRAIN_SAMPLES:\n",
    "        # On tire un sous-échantillon aléatoire sans remplacement\n",
    "        idx = np.random.choice(N_total, size=MAX_TRAIN_SAMPLES, replace=False)\n",
    "        obs_array = obs_array[idx]\n",
    "        act_array = act_array[idx]\n",
    "        print(f\"  → Sous-échantillonnage du buffer DAgger : {N_total} → {MAX_TRAIN_SAMPLES} exemples\")\n",
    "    else:\n",
    "        print(f\"  → Utilisation de tous les exemples : {N_total}\")\n",
    "\n",
    "    # 1) Le modèle MaskablePPO tourne en CPU → on aligne l'entraînement dessus\n",
    "    device = model.policy.device\n",
    "    policy = model.policy.to(device)\n",
    "\n",
    "    # 2) Conversion en tenseurs\n",
    "    X = torch.tensor(obs_array, dtype=torch.float32, device=device)\n",
    "    y = torch.tensor(act_array, dtype=torch.long, device=device)\n",
    "\n",
    "    # 3) Sécurité : les actions doivent être dans [0, action_space.n]\n",
    "    max_action = policy.action_space.n - 1\n",
    "    y = torch.clamp(y, 0, max_action)\n",
    "\n",
    "    # 4) Optimiseur + fonction de perte\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=1e-4)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    N = len(X)\n",
    "    nb_batches = (N + batch_size - 1) // batch_size\n",
    "\n",
    "    print(f\"  → Entraînement supervisé sur {N} exemples ({nb_batches} batches)\")\n",
    "\n",
    "    avg_epoch_loss = 0.0\n",
    "\n",
    "    # 5) Entraînement sur plusieurs epochs\n",
    "    for epoch in range(epochs):\n",
    "        perm = torch.randperm(N, device=device)\n",
    "        Xb = X[perm]\n",
    "        yb = y[perm]\n",
    "\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for i in range(nb_batches):\n",
    "            start = i * batch_size\n",
    "            end = min(start + batch_size, N)\n",
    "\n",
    "            xb_i = Xb[start:end]\n",
    "            yb_i = yb[start:end]\n",
    "\n",
    "            # 6) Forward pass (architecture PPO)\n",
    "            features = policy.extract_features(xb_i)             # encode l'état\n",
    "            pi_latent, _ = policy.mlp_extractor(features)        # réseau intermédiaire\n",
    "            logits = policy.action_net(pi_latent)                # logits actions\n",
    "\n",
    "            loss = loss_fn(logits, yb_i)\n",
    "\n",
    "            # 7) Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        epoch_loss = total_loss / nb_batches\n",
    "        avg_epoch_loss = epoch_loss\n",
    "\n",
    "        print(f\"    Epoch {epoch+1}/{epochs} — loss = {epoch_loss:.4f}\")\n",
    "\n",
    "    print(\"  ✓ Entraînement supervisé terminé.\\n\")\n",
    "    return avg_epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9cc816b-de85-454d-ac96-403a90b1418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 7 — Test de l'élève sur une semaine (version MlpPolicy)\n",
    "\n",
    "def run_student_episode(env, model, max_steps: int = 10080):\n",
    "    obs, info = env.reset()\n",
    "    total_reward = 0.0\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        # On donne simplement l'observation brute au modèle\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    return total_reward, t + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7b25bf2-537a-4335-b6cf-96375ab53aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELLULE 8 — Collecte des données DAgger \n",
    "# ============================================================\n",
    "\n",
    "def collect_dagger_data_from_student(model, max_steps=10080):\n",
    "    \"\"\"\n",
    "    L'élève joue un épisode complet.\n",
    "    L'expert corrige chaque action.\n",
    "    On renvoie :\n",
    "    - obs_list  : toutes les observations rencontrées\n",
    "    - act_list  : actions expertes correspondantes\n",
    "    - total_reward_student : reward cumulé du student\n",
    "    - nb_steps_student     : nombre de steps joués\n",
    "    \"\"\"\n",
    "\n",
    "    env = WorkshopEnv()\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    obs_list = []\n",
    "    act_list = []\n",
    "\n",
    "    total_reward = 0.0\n",
    "\n",
    "    for t in range(max_steps):\n",
    "\n",
    "        #  Très important : récupérer le MASQUE\n",
    "        mask = env.get_action_mask()\n",
    "\n",
    "        #  predict() DOIT recevoir action_masks pour éviter actions hors borne\n",
    "        action_student, _ = model.predict(\n",
    "            obs,\n",
    "            deterministic=True,\n",
    "            action_masks=mask\n",
    "        )\n",
    "\n",
    "        # L'expert corrige l'action\n",
    "        action_expert = expert_policy_masked(env, obs)\n",
    "\n",
    "        obs_list.append(obs.copy())\n",
    "        act_list.append(action_expert)\n",
    "\n",
    "        obs, reward, terminated, truncated, info = env.step(action_student)\n",
    "        total_reward += reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    return (\n",
    "        np.array(obs_list, dtype=np.float32),\n",
    "        np.array(act_list, dtype=np.int64),\n",
    "        float(total_reward),\n",
    "        t + 1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38933a77-7d01-47d7-a4dc-74ade72d09a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 9 — Une itération DAgger imitation-seule (SANS PPO)\n",
    "\n",
    "def dagger_hybrid_iteration(\n",
    "    model,\n",
    "    dagger_obs,\n",
    "    dagger_actions,\n",
    "    supervised_epochs: int = 3,\n",
    "    rl_timesteps: int = 10_000\n",
    "):\n",
    "    \"\"\"\n",
    "    VERSION DIAGNOSTIC : imitation supervisée SEULE\n",
    "    (on désactive PPO pour voir si le modèle apprend réellement les actions expertes).\n",
    "    Cette version enregistre les métriques dans le dict global `history`.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n===== Phase 1 : Imitation supervisée sur buffer DAgger =====\")\n",
    "    loss_value = train_student_supervised(model, dagger_obs, dagger_actions, epochs=supervised_epochs)\n",
    "    history[\"supervised_loss\"].append(loss_value)\n",
    "\n",
    "    print(\"\\n===== Phase 2 : (désactivée) =====\")\n",
    "    print(\"⚠ PPO désactivé volontairement pour test de diagnostic.\")\n",
    "\n",
    "    print(\"\\n===== Phase 3 : Collecte DAgger (élève + expert) =====\")\n",
    "    new_obs, new_actions, R_student, steps_student = collect_dagger_data_from_student(model)\n",
    "    history[\"reward_collect\"].append(R_student)\n",
    "\n",
    "    print(f\"Reward élève pendant collecte DAgger : {R_student:.2f} sur {steps_student} steps\")\n",
    "\n",
    "    # Agrégation au buffer\n",
    "    dagger_obs = np.vstack([dagger_obs, new_obs])\n",
    "    dagger_actions = np.concatenate([dagger_actions, new_actions])\n",
    "\n",
    "    print(\"Taille du buffer DAgger après agrégation :\", dagger_obs.shape)\n",
    "\n",
    "    return dagger_obs, dagger_actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52d9c0a1-7ad2-44b9-92da-610a329d9ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BOUCLE DAgger — VERSION PURE (Épisodes complets 10080 steps) ===\n",
      "\n",
      "\n",
      "=========== Iteration 1/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 1008000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 5.2554\n",
      "    Epoch 2/3 — loss = 5.1390\n",
      "    Epoch 3/3 — loss = 4.9769\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 4.9769\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -6053.44\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -5852.28\n",
      "✓ Historique partiel sauvegardé (itération 1)\n",
      " Temps écoulé : 0.5 min\n",
      "\n",
      "=========== Iteration 2/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 4.9407\n",
      "    Epoch 2/3 — loss = 4.7590\n",
      "    Epoch 3/3 — loss = 4.5415\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 4.5415\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -6157.94\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -5915.66\n",
      "✓ Historique partiel sauvegardé (itération 2)\n",
      " Temps écoulé : 1.0 min\n",
      "\n",
      "=========== Iteration 3/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 4.5247\n",
      "    Epoch 2/3 — loss = 4.2965\n",
      "    Epoch 3/3 — loss = 4.0465\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 4.0465\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -6132.84\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -6145.54\n",
      "✓ Historique partiel sauvegardé (itération 3)\n",
      " Temps écoulé : 1.6 min\n",
      "\n",
      "=========== Iteration 4/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 4.0207\n",
      "    Epoch 2/3 — loss = 3.7486\n",
      "    Epoch 3/3 — loss = 3.4634\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 3.4634\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -6139.00\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -5808.04\n",
      "✓ Historique partiel sauvegardé (itération 4)\n",
      " Temps écoulé : 2.1 min\n",
      "\n",
      "=========== Iteration 5/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 3.1777\n",
      "    Epoch 2/3 — loss = 2.8850\n",
      "    Epoch 3/3 — loss = 2.5953\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.5953\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -397359.30\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -396903.82\n",
      "✓ Historique partiel sauvegardé (itération 5)\n",
      "✓ Dataset sauvegardé (itération 5)\n",
      " Temps écoulé : 2.6 min\n",
      "\n",
      "=========== Iteration 6/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 3.3281\n",
      "    Epoch 2/3 — loss = 3.0847\n",
      "    Epoch 3/3 — loss = 2.8572\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.8572\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -397079.54\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -397213.94\n",
      "✓ Historique partiel sauvegardé (itération 6)\n",
      " Temps écoulé : 3.1 min\n",
      "\n",
      "=========== Iteration 7/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 3.6917\n",
      "    Epoch 2/3 — loss = 3.4940\n",
      "    Epoch 3/3 — loss = 3.2957\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 3.2957\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -397037.74\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -397437.72\n",
      "✓ Historique partiel sauvegardé (itération 7)\n",
      " Temps écoulé : 3.7 min\n",
      "\n",
      "=========== Iteration 8/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 4.0940\n",
      "    Epoch 2/3 — loss = 3.8228\n",
      "    Epoch 3/3 — loss = 3.5481\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 3.5481\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -397049.04\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -397267.52\n",
      "✓ Historique partiel sauvegardé (itération 8)\n",
      " Temps écoulé : 4.2 min\n",
      "\n",
      "=========== Iteration 9/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 3.2766\n",
      "    Epoch 2/3 — loss = 3.0102\n",
      "    Epoch 3/3 — loss = 2.7534\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.7534\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -397120.12\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -397301.16\n",
      "✓ Historique partiel sauvegardé (itération 9)\n",
      " Temps écoulé : 4.8 min\n",
      "\n",
      "=========== Iteration 10/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.5051\n",
      "    Epoch 2/3 — loss = 2.2706\n",
      "    Epoch 3/3 — loss = 2.0502\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.0502\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -397250.18\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -396723.42\n",
      "✓ Historique partiel sauvegardé (itération 10)\n",
      "✓ Dataset sauvegardé (itération 10)\n",
      " Temps écoulé : 5.4 min\n",
      "\n",
      "=========== Iteration 11/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 1.8449\n",
      "    Epoch 2/3 — loss = 1.6501\n",
      "    Epoch 3/3 — loss = 1.4836\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 1.4836\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -384205.92\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -384233.84\n",
      "✓ Historique partiel sauvegardé (itération 11)\n",
      " Temps écoulé : 5.9 min\n",
      "\n",
      "=========== Iteration 12/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 3.0052\n",
      "    Epoch 2/3 — loss = 2.8581\n",
      "    Epoch 3/3 — loss = 2.7376\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.7376\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -384467.40\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -384159.22\n",
      "✓ Historique partiel sauvegardé (itération 12)\n",
      " Temps écoulé : 6.5 min\n",
      "\n",
      "=========== Iteration 13/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 4.1895\n",
      "    Epoch 2/3 — loss = 3.9944\n",
      "    Epoch 3/3 — loss = 3.8011\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 3.8011\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -384329.20\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -384136.64\n",
      "✓ Historique partiel sauvegardé (itération 13)\n",
      " Temps écoulé : 7.0 min\n",
      "\n",
      "=========== Iteration 14/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 4.8637\n",
      "    Epoch 2/3 — loss = 4.5596\n",
      "    Epoch 3/3 — loss = 4.2652\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 4.2652\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -384062.50\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -384117.64\n",
      "✓ Historique partiel sauvegardé (itération 14)\n",
      " Temps écoulé : 7.6 min\n",
      "\n",
      "=========== Iteration 15/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 3.9828\n",
      "    Epoch 2/3 — loss = 3.7089\n",
      "    Epoch 3/3 — loss = 3.4572\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 3.4572\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -383877.42\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -384169.10\n",
      "✓ Historique partiel sauvegardé (itération 15)\n",
      "✓ Dataset sauvegardé (itération 15)\n",
      " Temps écoulé : 8.1 min\n",
      "\n",
      "=========== Iteration 16/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 3.2214\n",
      "    Epoch 2/3 — loss = 2.9841\n",
      "    Epoch 3/3 — loss = 2.7467\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.7467\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -359166.36\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -359058.36\n",
      "✓ Historique partiel sauvegardé (itération 16)\n",
      " Temps écoulé : 8.7 min\n",
      "\n",
      "=========== Iteration 17/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 3.1917\n",
      "    Epoch 2/3 — loss = 2.9629\n",
      "    Epoch 3/3 — loss = 2.7540\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.7540\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -358494.36\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -358957.78\n",
      "✓ Historique partiel sauvegardé (itération 17)\n",
      " Temps écoulé : 9.3 min\n",
      "\n",
      "=========== Iteration 18/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 3.1595\n",
      "    Epoch 2/3 — loss = 2.9495\n",
      "    Epoch 3/3 — loss = 2.7523\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.7523\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -358975.38\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -358932.00\n",
      "✓ Historique partiel sauvegardé (itération 18)\n",
      " Temps écoulé : 9.8 min\n",
      "\n",
      "=========== Iteration 19/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 3.0574\n",
      "    Epoch 2/3 — loss = 2.7552\n",
      "    Epoch 3/3 — loss = 2.4567\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.4567\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -115100.26\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -115047.60\n",
      "✓ Historique partiel sauvegardé (itération 19)\n",
      " Temps écoulé : 10.3 min\n",
      "\n",
      "=========== Iteration 20/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.1075\n",
      "    Epoch 2/3 — loss = 1.8434\n",
      "    Epoch 3/3 — loss = 1.5963\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 1.5963\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -5950.46\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -5579.76\n",
      "✓ Historique partiel sauvegardé (itération 20)\n",
      "✓ Dataset sauvegardé (itération 20)\n",
      " Temps écoulé : 10.9 min\n",
      "\n",
      "=========== Iteration 21/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 1.6319\n",
      "    Epoch 2/3 — loss = 1.4688\n",
      "    Epoch 3/3 — loss = 1.3391\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 1.3391\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -6220.16\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -6051.08\n",
      "✓ Historique partiel sauvegardé (itération 21)\n",
      " Temps écoulé : 11.4 min\n",
      "\n",
      "=========== Iteration 22/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 1.4780\n",
      "    Epoch 2/3 — loss = 1.3652\n",
      "    Epoch 3/3 — loss = 1.2769\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 1.2769\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -117770.02\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -117330.34\n",
      "✓ Historique partiel sauvegardé (itération 22)\n",
      " Temps écoulé : 11.9 min\n",
      "\n",
      "=========== Iteration 23/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 1.7490\n",
      "    Epoch 2/3 — loss = 1.6112\n",
      "    Epoch 3/3 — loss = 1.5019\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 1.5019\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -232519.52\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -233540.64\n",
      "✓ Historique partiel sauvegardé (itération 23)\n",
      " Temps écoulé : 12.4 min\n",
      "\n",
      "=========== Iteration 24/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 1.8034\n",
      "    Epoch 2/3 — loss = 1.6489\n",
      "    Epoch 3/3 — loss = 1.5112\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 1.5112\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -225242.30\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -224898.20\n",
      "✓ Historique partiel sauvegardé (itération 24)\n",
      " Temps écoulé : 12.9 min\n",
      "\n",
      "=========== Iteration 25/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.0151\n",
      "    Epoch 2/3 — loss = 1.8540\n",
      "    Epoch 3/3 — loss = 1.7302\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 1.7302\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -176218.58\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -176740.66\n",
      "✓ Historique partiel sauvegardé (itération 25)\n",
      "✓ Dataset sauvegardé (itération 25)\n",
      " Temps écoulé : 13.4 min\n",
      "\n",
      "=========== Iteration 26/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.0608\n",
      "    Epoch 2/3 — loss = 1.8918\n",
      "    Epoch 3/3 — loss = 1.7602\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 1.7602\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -163446.42\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -163268.20\n",
      "✓ Historique partiel sauvegardé (itération 26)\n",
      " Temps écoulé : 14.0 min\n",
      "\n",
      "=========== Iteration 27/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 1.8804\n",
      "    Epoch 2/3 — loss = 1.7222\n",
      "    Epoch 3/3 — loss = 1.5914\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 1.5914\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -27670.94\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -25281.40\n",
      "✓ Historique partiel sauvegardé (itération 27)\n",
      " Temps écoulé : 14.5 min\n",
      "\n",
      "=========== Iteration 28/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 1.7280\n",
      "    Epoch 2/3 — loss = 1.5947\n",
      "    Epoch 3/3 — loss = 1.4889\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 1.4889\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -5977.52\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -6159.42\n",
      "✓ Historique partiel sauvegardé (itération 28)\n",
      " Temps écoulé : 14.9 min\n",
      "\n",
      "=========== Iteration 29/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 1.5410\n",
      "    Epoch 2/3 — loss = 1.4040\n",
      "    Epoch 3/3 — loss = 1.2971\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 1.2971\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -5702.36\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -5872.96\n",
      "✓ Historique partiel sauvegardé (itération 29)\n",
      " Temps écoulé : 15.4 min\n",
      "\n",
      "=========== Iteration 30/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 1.4472\n",
      "    Epoch 2/3 — loss = 1.3269\n",
      "    Epoch 3/3 — loss = 1.2427\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 1.2427\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -83637.84\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -83619.76\n",
      "✓ Historique partiel sauvegardé (itération 30)\n",
      "✓ Dataset sauvegardé (itération 30)\n",
      " Temps écoulé : 15.9 min\n",
      "\n",
      "=========== Iteration 31/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 1.6051\n",
      "    Epoch 2/3 — loss = 1.4699\n",
      "    Epoch 3/3 — loss = 1.3579\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 1.3579\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -122877.68\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -123196.98\n",
      "✓ Historique partiel sauvegardé (itération 31)\n",
      " Temps écoulé : 16.4 min\n",
      "\n",
      "=========== Iteration 32/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 1.6430\n",
      "    Epoch 2/3 — loss = 1.4844\n",
      "    Epoch 3/3 — loss = 1.3556\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 1.3556\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -87500.82\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -87711.24\n",
      "✓ Historique partiel sauvegardé (itération 32)\n",
      " Temps écoulé : 16.9 min\n",
      "\n",
      "=========== Iteration 33/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 1.7543\n",
      "    Epoch 2/3 — loss = 1.6092\n",
      "    Epoch 3/3 — loss = 1.5031\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 1.5031\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -91017.42\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -91523.24\n",
      "✓ Historique partiel sauvegardé (itération 33)\n",
      " Temps écoulé : 17.4 min\n",
      "\n",
      "=========== Iteration 34/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 1.6469\n",
      "    Epoch 2/3 — loss = 1.5616\n",
      "    Epoch 3/3 — loss = 1.4973\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 1.4973\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -5068.78\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -4610.22\n",
      "✓ Historique partiel sauvegardé (itération 34)\n",
      " Temps écoulé : 17.9 min\n",
      "\n",
      "=========== Iteration 35/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.4162\n",
      "    Epoch 2/3 — loss = 2.2683\n",
      "    Epoch 3/3 — loss = 2.1582\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.1582\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -5936.36\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -5966.78\n",
      "✓ Historique partiel sauvegardé (itération 35)\n",
      "✓ Dataset sauvegardé (itération 35)\n",
      " Temps écoulé : 18.4 min\n",
      "\n",
      "=========== Iteration 36/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.7769\n",
      "    Epoch 2/3 — loss = 2.6327\n",
      "    Epoch 3/3 — loss = 2.5214\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.5214\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -6970.22\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -7027.20\n",
      "✓ Historique partiel sauvegardé (itération 36)\n",
      " Temps écoulé : 18.9 min\n",
      "\n",
      "=========== Iteration 37/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.8238\n",
      "    Epoch 2/3 — loss = 2.6939\n",
      "    Epoch 3/3 — loss = 2.5866\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.5866\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -8374.84\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -9316.08\n",
      "✓ Historique partiel sauvegardé (itération 37)\n",
      " Temps écoulé : 19.4 min\n",
      "\n",
      "=========== Iteration 38/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.6848\n",
      "    Epoch 2/3 — loss = 2.5788\n",
      "    Epoch 3/3 — loss = 2.4992\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.4992\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -36748.72\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -37229.84\n",
      "✓ Historique partiel sauvegardé (itération 38)\n",
      " Temps écoulé : 19.9 min\n",
      "\n",
      "=========== Iteration 39/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.6103\n",
      "    Epoch 2/3 — loss = 2.5015\n",
      "    Epoch 3/3 — loss = 2.4207\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.4207\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 10006.70\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 10339.52\n",
      "✓ Historique partiel sauvegardé (itération 39)\n",
      " Temps écoulé : 20.4 min\n",
      "\n",
      "=========== Iteration 40/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 3.1456\n",
      "    Epoch 2/3 — loss = 3.0280\n",
      "    Epoch 3/3 — loss = 2.9315\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.9315\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -5371.36\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -5491.36\n",
      "✓ Historique partiel sauvegardé (itération 40)\n",
      "✓ Dataset sauvegardé (itération 40)\n",
      " Temps écoulé : 20.9 min\n",
      "\n",
      "=========== Iteration 41/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 4.1393\n",
      "    Epoch 2/3 — loss = 3.9206\n",
      "    Epoch 3/3 — loss = 3.7547\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 3.7547\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -5956.88\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -6181.94\n",
      "✓ Historique partiel sauvegardé (itération 41)\n",
      " Temps écoulé : 21.5 min\n",
      "\n",
      "=========== Iteration 42/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 3.9403\n",
      "    Epoch 2/3 — loss = 3.7931\n",
      "    Epoch 3/3 — loss = 3.6613\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 3.6613\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -6497.86\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -6514.70\n",
      "✓ Historique partiel sauvegardé (itération 42)\n",
      " Temps écoulé : 22.0 min\n",
      "\n",
      "=========== Iteration 43/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 3.2601\n",
      "    Epoch 2/3 — loss = 3.1163\n",
      "    Epoch 3/3 — loss = 3.0014\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 3.0014\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -4626.04\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -6019.54\n",
      "✓ Historique partiel sauvegardé (itération 43)\n",
      " Temps écoulé : 22.5 min\n",
      "\n",
      "=========== Iteration 44/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.1906\n",
      "    Epoch 2/3 — loss = 2.0385\n",
      "    Epoch 3/3 — loss = 1.9440\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 1.9440\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 5206.58\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 4519.20\n",
      "✓ Historique partiel sauvegardé (itération 44)\n",
      " Temps écoulé : 23.0 min\n",
      "\n",
      "=========== Iteration 45/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 1.9621\n",
      "    Epoch 2/3 — loss = 1.8656\n",
      "    Epoch 3/3 — loss = 1.7992\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 1.7992\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 11066.64\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 11250.92\n",
      "✓ Historique partiel sauvegardé (itération 45)\n",
      "✓ Dataset sauvegardé (itération 45)\n",
      " Temps écoulé : 23.5 min\n",
      "\n",
      "=========== Iteration 46/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.6577\n",
      "    Epoch 2/3 — loss = 2.5490\n",
      "    Epoch 3/3 — loss = 2.4600\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.4600\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 11149.14\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 11273.48\n",
      "✓ Historique partiel sauvegardé (itération 46)\n",
      " Temps écoulé : 24.0 min\n",
      "\n",
      "=========== Iteration 47/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 3.3670\n",
      "    Epoch 2/3 — loss = 3.2203\n",
      "    Epoch 3/3 — loss = 3.1238\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 3.1238\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 10912.30\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 10493.90\n",
      "✓ Historique partiel sauvegardé (itération 47)\n",
      " Temps écoulé : 24.5 min\n",
      "\n",
      "=========== Iteration 48/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 3.8399\n",
      "    Epoch 2/3 — loss = 3.6762\n",
      "    Epoch 3/3 — loss = 3.5678\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 3.5678\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 5357.36\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 6299.98\n",
      "✓ Historique partiel sauvegardé (itération 48)\n",
      " Temps écoulé : 25.0 min\n",
      "\n",
      "=========== Iteration 49/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 3.6317\n",
      "    Epoch 2/3 — loss = 3.5415\n",
      "    Epoch 3/3 — loss = 3.4708\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 3.4708\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 5714.22\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 6721.76\n",
      "✓ Historique partiel sauvegardé (itération 49)\n",
      " Temps écoulé : 25.5 min\n",
      "\n",
      "=========== Iteration 50/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 3.0579\n",
      "    Epoch 2/3 — loss = 2.9666\n",
      "    Epoch 3/3 — loss = 2.8985\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.8985\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 2959.36\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 4230.18\n",
      "✓ Historique partiel sauvegardé (itération 50)\n",
      "✓ Dataset sauvegardé (itération 50)\n",
      " Temps écoulé : 26.0 min\n",
      "\n",
      "=========== Iteration 51/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.7604\n",
      "    Epoch 2/3 — loss = 2.6995\n",
      "    Epoch 3/3 — loss = 2.6541\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.6541\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -2205.22\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -1873.36\n",
      "✓ Historique partiel sauvegardé (itération 51)\n",
      " Temps écoulé : 26.5 min\n",
      "\n",
      "=========== Iteration 52/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.6412\n",
      "    Epoch 2/3 — loss = 2.5906\n",
      "    Epoch 3/3 — loss = 2.5507\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.5507\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -6563.58\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -6282.72\n",
      "✓ Historique partiel sauvegardé (itération 52)\n",
      " Temps écoulé : 27.0 min\n",
      "\n",
      "=========== Iteration 53/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.7235\n",
      "    Epoch 2/3 — loss = 2.6075\n",
      "    Epoch 3/3 — loss = 2.5162\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.5162\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -27658.52\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -27845.90\n",
      "✓ Historique partiel sauvegardé (itération 53)\n",
      " Temps écoulé : 27.5 min\n",
      "\n",
      "=========== Iteration 54/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.3994\n",
      "    Epoch 2/3 — loss = 2.2858\n",
      "    Epoch 3/3 — loss = 2.2077\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.2077\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : -17981.30\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : -17011.34\n",
      "✓ Historique partiel sauvegardé (itération 54)\n",
      " Temps écoulé : 28.0 min\n",
      "\n",
      "=========== Iteration 55/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.2084\n",
      "    Epoch 2/3 — loss = 2.0991\n",
      "    Epoch 3/3 — loss = 2.0171\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.0171\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 7543.90\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 7921.96\n",
      "✓ Historique partiel sauvegardé (itération 55)\n",
      "✓ Dataset sauvegardé (itération 55)\n",
      " Temps écoulé : 28.5 min\n",
      "\n",
      "=========== Iteration 56/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.2949\n",
      "    Epoch 2/3 — loss = 2.1875\n",
      "    Epoch 3/3 — loss = 2.0936\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.0936\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 14447.96\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 14278.98\n",
      "✓ Historique partiel sauvegardé (itération 56)\n",
      " Temps écoulé : 29.0 min\n",
      "\n",
      "=========== Iteration 57/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.8574\n",
      "    Epoch 2/3 — loss = 2.7179\n",
      "    Epoch 3/3 — loss = 2.6216\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.6216\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 12378.28\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 12591.56\n",
      "✓ Historique partiel sauvegardé (itération 57)\n",
      " Temps écoulé : 29.5 min\n",
      "\n",
      "=========== Iteration 58/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.8881\n",
      "    Epoch 2/3 — loss = 2.8070\n",
      "    Epoch 3/3 — loss = 2.7437\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.7437\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 11341.30\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 11013.24\n",
      "✓ Historique partiel sauvegardé (itération 58)\n",
      " Temps écoulé : 30.0 min\n",
      "\n",
      "=========== Iteration 59/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 3.2709\n",
      "    Epoch 2/3 — loss = 3.1706\n",
      "    Epoch 3/3 — loss = 3.1150\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 3.1150\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 9400.26\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 10348.30\n",
      "✓ Historique partiel sauvegardé (itération 59)\n",
      " Temps écoulé : 30.5 min\n",
      "\n",
      "=========== Iteration 60/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.8857\n",
      "    Epoch 2/3 — loss = 2.8276\n",
      "    Epoch 3/3 — loss = 2.7788\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.7788\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 3122.98\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 3558.72\n",
      "✓ Historique partiel sauvegardé (itération 60)\n",
      "✓ Dataset sauvegardé (itération 60)\n",
      " Temps écoulé : 31.0 min\n",
      "\n",
      "=========== Iteration 61/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.7637\n",
      "    Epoch 2/3 — loss = 2.6764\n",
      "    Epoch 3/3 — loss = 2.6055\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.6055\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 10723.70\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 11643.82\n",
      "✓ Historique partiel sauvegardé (itération 61)\n",
      " Temps écoulé : 31.5 min\n",
      "\n",
      "=========== Iteration 62/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.3937\n",
      "    Epoch 2/3 — loss = 2.3468\n",
      "    Epoch 3/3 — loss = 2.3128\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.3128\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 10972.82\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 11238.40\n",
      "✓ Historique partiel sauvegardé (itération 62)\n",
      " Temps écoulé : 32.0 min\n",
      "\n",
      "=========== Iteration 63/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.4284\n",
      "    Epoch 2/3 — loss = 2.3719\n",
      "    Epoch 3/3 — loss = 2.3268\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.3268\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 10319.38\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 12119.06\n",
      "✓ Historique partiel sauvegardé (itération 63)\n",
      " Temps écoulé : 32.5 min\n",
      "\n",
      "=========== Iteration 64/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.1712\n",
      "    Epoch 2/3 — loss = 2.0936\n",
      "    Epoch 3/3 — loss = 2.0414\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.0414\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 13155.78\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 12762.74\n",
      "✓ Historique partiel sauvegardé (itération 64)\n",
      " Temps écoulé : 33.0 min\n",
      "\n",
      "=========== Iteration 65/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 1.5876\n",
      "    Epoch 2/3 — loss = 1.5032\n",
      "    Epoch 3/3 — loss = 1.4574\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 1.4574\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 10630.22\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 10548.16\n",
      "✓ Historique partiel sauvegardé (itération 65)\n",
      "✓ Dataset sauvegardé (itération 65)\n",
      " Temps écoulé : 33.5 min\n",
      "\n",
      "=========== Iteration 66/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 1.7528\n",
      "    Epoch 2/3 — loss = 1.6852\n",
      "    Epoch 3/3 — loss = 1.6351\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 1.6351\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 2572.46\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 2597.00\n",
      "✓ Historique partiel sauvegardé (itération 66)\n",
      " Temps écoulé : 33.9 min\n",
      "\n",
      "=========== Iteration 67/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.4520\n",
      "    Epoch 2/3 — loss = 2.3621\n",
      "    Epoch 3/3 — loss = 2.3102\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.3102\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 8485.40\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 8239.22\n",
      "✓ Historique partiel sauvegardé (itération 67)\n",
      " Temps écoulé : 34.4 min\n",
      "\n",
      "=========== Iteration 68/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.8819\n",
      "    Epoch 2/3 — loss = 2.7975\n",
      "    Epoch 3/3 — loss = 2.7352\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.7352\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 8023.70\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 8258.26\n",
      "✓ Historique partiel sauvegardé (itération 68)\n",
      " Temps écoulé : 34.9 min\n",
      "\n",
      "=========== Iteration 69/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.4641\n",
      "    Epoch 2/3 — loss = 2.4106\n",
      "    Epoch 3/3 — loss = 2.3611\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.3611\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 12541.42\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 11101.48\n",
      "✓ Historique partiel sauvegardé (itération 69)\n",
      " Temps écoulé : 35.4 min\n",
      "\n",
      "=========== Iteration 70/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 1.9476\n",
      "    Epoch 2/3 — loss = 1.8633\n",
      "    Epoch 3/3 — loss = 1.8132\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 1.8132\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 13399.86\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 12527.86\n",
      "✓ Historique partiel sauvegardé (itération 70)\n",
      "✓ Dataset sauvegardé (itération 70)\n",
      " Temps écoulé : 35.9 min\n",
      "\n",
      "=========== Iteration 71/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 1.1065\n",
      "    Epoch 2/3 — loss = 1.0061\n",
      "    Epoch 3/3 — loss = 0.9400\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 0.9400\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 11075.46\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 11099.10\n",
      "✓ Historique partiel sauvegardé (itération 71)\n",
      " Temps écoulé : 36.4 min\n",
      "\n",
      "=========== Iteration 72/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 1.9287\n",
      "    Epoch 2/3 — loss = 1.8476\n",
      "    Epoch 3/3 — loss = 1.7691\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 1.7691\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 8652.94\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 9013.10\n",
      "✓ Historique partiel sauvegardé (itération 72)\n",
      " Temps écoulé : 36.9 min\n",
      "\n",
      "=========== Iteration 73/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.7642\n",
      "    Epoch 2/3 — loss = 2.6690\n",
      "    Epoch 3/3 — loss = 2.5997\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.5997\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 11567.80\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 12115.02\n",
      "✓ Historique partiel sauvegardé (itération 73)\n",
      " Temps écoulé : 37.4 min\n",
      "\n",
      "=========== Iteration 74/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.9812\n",
      "    Epoch 2/3 — loss = 2.8980\n",
      "    Epoch 3/3 — loss = 2.8501\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.8501\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 12500.84\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 11923.84\n",
      "✓ Historique partiel sauvegardé (itération 74)\n",
      " Temps écoulé : 37.9 min\n",
      "\n",
      "=========== Iteration 75/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.3132\n",
      "    Epoch 2/3 — loss = 2.2534\n",
      "    Epoch 3/3 — loss = 2.2110\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.2110\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 11628.22\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 11282.04\n",
      "✓ Historique partiel sauvegardé (itération 75)\n",
      "✓ Dataset sauvegardé (itération 75)\n",
      " Temps écoulé : 38.4 min\n",
      "\n",
      "=========== Iteration 76/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 1.9226\n",
      "    Epoch 2/3 — loss = 1.8502\n",
      "    Epoch 3/3 — loss = 1.7924\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 1.7924\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 7198.14\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 7309.72\n",
      "✓ Historique partiel sauvegardé (itération 76)\n",
      " Temps écoulé : 38.9 min\n",
      "\n",
      "=========== Iteration 77/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.5319\n",
      "    Epoch 2/3 — loss = 2.4752\n",
      "    Epoch 3/3 — loss = 2.4345\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.4345\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 10400.00\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 10766.72\n",
      "✓ Historique partiel sauvegardé (itération 77)\n",
      " Temps écoulé : 39.4 min\n",
      "\n",
      "=========== Iteration 78/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 3.2587\n",
      "    Epoch 2/3 — loss = 3.1661\n",
      "    Epoch 3/3 — loss = 3.1045\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 3.1045\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 11406.46\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 11303.98\n",
      "✓ Historique partiel sauvegardé (itération 78)\n",
      " Temps écoulé : 39.8 min\n",
      "\n",
      "=========== Iteration 79/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 3.0390\n",
      "    Epoch 2/3 — loss = 2.9974\n",
      "    Epoch 3/3 — loss = 2.9572\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.9572\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 11424.20\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 12708.72\n",
      "✓ Historique partiel sauvegardé (itération 79)\n",
      " Temps écoulé : 40.3 min\n",
      "\n",
      "=========== Iteration 80/80 DAgger ===========\n",
      "\n",
      "[Phase 1] Entraînement supervisé (imitation expert)\n",
      "  → Sous-échantillonnage du buffer DAgger : 30000 → 12000 exemples\n",
      "  → Entraînement supervisé sur 12000 exemples (24 batches)\n",
      "    Epoch 1/3 — loss = 2.9330\n",
      "    Epoch 2/3 — loss = 2.8871\n",
      "    Epoch 3/3 — loss = 2.8399\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "  → Loss moyenne = 2.8399\n",
      "\n",
      "[Phase 2] Collecte DAgger (1 semaine simulée)\n",
      "  → Reward élève sur 10080 steps : 11152.70\n",
      "  → Steps joués : 10080\n",
      "  → Taille du buffer DAgger : (30000, 23)\n",
      "\n",
      "[Phase 3] Évaluation (1 semaine complète, 10080 steps)\n",
      "  → Reward élève en évaluation (10080 steps) : 10947.54\n",
      "✓ Historique partiel sauvegardé (itération 80)\n",
      "✓ Dataset sauvegardé (itération 80)\n",
      " Temps écoulé : 40.8 min\n",
      "\n",
      "=== FIN DU DAgger — VERSION ÉPISODES 10080 STEPS ===\n",
      "\n",
      "\n",
      "✓ Modèle final sauvegardé : student_dagger_final.zip\n"
     ]
    }
   ],
   "source": [
    "# ======================= CELLULE 10 — DAgger PUR (10080 STEPS) =======================\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Hyperparamètres\n",
    "N_ITERS            = 80         # itérations DAgger \n",
    "SUPERVISED_EPOCHS  = 3\n",
    "BATCH_SIZE         = 512\n",
    "EPISODE_STEPS      = 10080     # 1 semaine = 10080 steps\n",
    "MAX_BUFFER         = 30_000    # borne RAM (à ajuster selon ta machine)\n",
    "\n",
    "history = {\n",
    "    \"supervised_loss\": [],\n",
    "    \"reward_collect\": [],\n",
    "    \"reward_eval\": []\n",
    "}\n",
    "\n",
    "print(\"\\n=== BOUCLE DAgger — VERSION PURE (Épisodes complets 10080 steps) ===\\n\")\n",
    "\n",
    "\n",
    "for it in range(1, N_ITERS + 1):\n",
    "\n",
    "    print(f\"\\n=========== Iteration {it}/{N_ITERS} DAgger ===========\")\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # PHASE 1 — Imitation supervisée sur le buffer actuel\n",
    "    # ----------------------------------------------------------\n",
    "    print(\"\\n[Phase 1] Entraînement supervisé (imitation expert)\")\n",
    "\n",
    "    avg_loss = train_student_supervised(\n",
    "        model_student,\n",
    "        dagger_obs,\n",
    "        dagger_actions,\n",
    "        epochs=SUPERVISED_EPOCHS,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    history[\"supervised_loss\"].append(avg_loss)\n",
    "    print(f\"  → Loss moyenne = {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # PHASE 2 — Collecte d'une SEMAINE complète avec l'élève\n",
    "    #            + corrections de l'expert (DAgger pur)\n",
    "    # ----------------------------------------------------------\n",
    "    print(\"\\n[Phase 2] Collecte DAgger (1 semaine simulée)\")\n",
    "\n",
    "    new_obs, new_actions, R_collect, steps = collect_dagger_data_from_student(\n",
    "        model_student,\n",
    "        max_steps=EPISODE_STEPS\n",
    "    )\n",
    "\n",
    "    print(f\"  → Reward élève sur 10080 steps : {R_collect:.2f}\")\n",
    "    print(f\"  → Steps joués : {steps}\")\n",
    "\n",
    "    history[\"reward_collect\"].append(R_collect)\n",
    "\n",
    "    # Ajout au buffer\n",
    "    dagger_obs = np.vstack([dagger_obs, new_obs])\n",
    "    dagger_actions = np.hstack([dagger_actions, new_actions])\n",
    "\n",
    "    # Sécurité mémoire : on ne garde que les MAX_BUFFER derniers exemples\n",
    "    if dagger_obs.shape[0] > MAX_BUFFER:\n",
    "        dagger_obs = dagger_obs[-MAX_BUFFER:]\n",
    "        dagger_actions = dagger_actions[-MAX_BUFFER:]\n",
    "\n",
    "    print(\"  → Taille du buffer DAgger :\", dagger_obs.shape)\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # PHASE 3 — Évaluation de l'élève sur 1 semaine complète\n",
    "    # ----------------------------------------------------------\n",
    "    print(\"\\n[Phase 3] Évaluation (1 semaine complète, 10080 steps)\")\n",
    "\n",
    "    env_eval = WorkshopEnv()\n",
    "    obs, info = env_eval.reset()\n",
    "    R_eval = 0\n",
    "\n",
    "    for _ in range(EPISODE_STEPS):\n",
    "\n",
    "        mask = env_eval.get_action_mask()\n",
    "        action, _ = model_student.predict(obs, deterministic=True, action_masks=mask)\n",
    "\n",
    "        obs, r, terminated, truncated, _ = env_eval.step(action)\n",
    "        R_eval += r\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    history[\"reward_eval\"].append(R_eval)\n",
    "\n",
    "    print(f\"  → Reward élève en évaluation (10080 steps) : {R_eval:.2f}\")\n",
    "   \n",
    "    # Sauvegarde de l'historique à chaque itération (sécurisé)\n",
    "    with open(\"history_dagger.json\", \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "\n",
    "    print(f\"✓ Historique partiel sauvegardé (itération {it})\")\n",
    "\n",
    "    # Sauvegarde du modèle élève pour cette itération\n",
    "    model_student.save(f\"student_dagger_iter_{it}.zip\")\n",
    "\n",
    "    # Sauvegarde du dataset toutes les 5 itérations\n",
    "    if it % 5 == 0:\n",
    "        np.save(f\"dagger_obs_iter_{it}.npy\", dagger_obs)\n",
    "        np.save(f\"dagger_actions_iter_{it}.npy\", dagger_actions)\n",
    "        print(f\"✓ Dataset sauvegardé (itération {it})\")\n",
    "\n",
    "    # Chrono simple\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\" Temps écoulé : {elapsed/60:.1f} min\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n=== FIN DU DAgger — VERSION ÉPISODES 10080 STEPS ===\\n\")\n",
    "# Sauvegarde finale une fois le DAgger terminé\n",
    "model_student.save(\"student_dagger_final.zip\")\n",
    "print(\"\\n✓ Modèle final sauvegardé : student_dagger_final.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92ffd74-0d35-4684-8062-cffd445cb307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:qlearning]",
   "language": "python",
   "name": "conda-env-qlearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
