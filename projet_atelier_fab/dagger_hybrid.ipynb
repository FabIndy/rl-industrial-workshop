{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e752a35-c9d5-492d-81ea-19f5c15f7352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1aac52cc9f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cellule 1 — Imports et configuration de base\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "from sb3_contrib import MaskablePPO\n",
    "from env.workshop_env import WorkshopEnv\n",
    "\n",
    "# Pour rendre les choses un peu reproductibles\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b89e3090-cdb1-448a-8bf7-70426116a22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 2 — Politique experte v2 compatible obs=23 variables + normalisation\n",
    "\n",
    "from env.workshop_env import WorkshopEnv\n",
    "import numpy as np\n",
    "\n",
    "def expert_policy(obs: np.ndarray, env: WorkshopEnv) -> int:\n",
    "    \"\"\"\n",
    "    Politique experte v2 avec obs normalisées (23 features).\n",
    "\n",
    "    On reconstruit d'abord les variables \"réelles\" à partir de l'observation\n",
    "    normalisée, en inversant exactement les formules de WorkshopEnv._get_obs(),\n",
    "    puis on applique la même logique experte qu'avant.\n",
    "\n",
    "    Rappel de la normalisation dans WorkshopEnv._get_obs() :\n",
    "      - time              -> obs[0] = time / max_time\n",
    "      - m1_busy           -> obs[1] = m1.busy\n",
    "      - m1_time_left      -> obs[2] = m1.time_left / 100\n",
    "      - m2_busy           -> obs[3] = m2.busy\n",
    "      - m2_time_left      -> obs[4] = m2.time_left / 100\n",
    "      - stock_raw         -> obs[5] = stock.raw / raw_capacity\n",
    "      - stock_p1          -> obs[6] = stock.p1 / raw_capacity\n",
    "      - stock_p2_inter    -> obs[7] = stock.p2_inter / raw_capacity\n",
    "      - stock_p2          -> obs[8] = stock.p2 / raw_capacity\n",
    "      - next_delivery_cd  -> obs[9] = next_delivery_countdown / 10080\n",
    "      - demande_p1        -> obs[10] = demande_p1 / 1000\n",
    "      - demande_p2        -> obs[11] = demande_p2 / 1000\n",
    "      - q_raw_incoming    -> obs[12] = q_total / 1000\n",
    "      - obs[13:23] = features supplémentaires (non utilisées ici)\n",
    "    \"\"\"\n",
    "\n",
    "    # ============================\n",
    "    # 1) Dé-normalisation\n",
    "    # ============================\n",
    "\n",
    "    # Temps\n",
    "    time = float(obs[0]) * float(env.max_time)\n",
    "\n",
    "    # États des machines\n",
    "    m1_busy = int(round(obs[1]))\n",
    "    m1_time_left = float(obs[2]) * 100.0\n",
    "\n",
    "    m2_busy = int(round(obs[3]))\n",
    "    m2_time_left = float(obs[4]) * 100.0\n",
    "\n",
    "    # Stocks (raw_capacity est connu dans l'env)\n",
    "    stock_raw = float(obs[5]) * float(env.raw_capacity)\n",
    "    stock_p1 = float(obs[6]) * float(env.raw_capacity)\n",
    "    stock_p2_inter = float(obs[7]) * float(env.raw_capacity)\n",
    "    stock_p2 = float(obs[8]) * float(env.raw_capacity)\n",
    "\n",
    "    # Prochaine livraison (normalisée sur 10080)\n",
    "    next_delivery_cd = float(obs[9]) * 10080.0\n",
    "\n",
    "    # Backlogs et MP en route (échelle 1000 comme dans _get_obs)\n",
    "    demande_p1 = float(obs[10]) * 1000.0\n",
    "    demande_p2 = float(obs[11]) * 1000.0\n",
    "    q_raw_incoming = float(obs[12]) * 1000.0\n",
    "\n",
    "    # Les 10 features supplémentaires obs[13:23] sont ignorées pour l’expert v2.\n",
    "\n",
    "    # Flags machine libres\n",
    "    m1_free = (m1_busy == 0)\n",
    "    m2_free = (m2_busy == 0)\n",
    "\n",
    "    # ============================\n",
    "    # 2) Logique experte d'origine\n",
    "    # ============================\n",
    "\n",
    "    # Règle 1 : si peu de MP (stock + en route), on commande\n",
    "    if stock_raw + q_raw_incoming < 20:\n",
    "        q_cmd = 10\n",
    "        return 149 + q_cmd  # action de commande MP (k = q_cmd)\n",
    "\n",
    "    # Règle 2 : si M2 libre et on a du P2_inter, on fait STEP2 pour finir P2\n",
    "    if m2_free and stock_p2_inter > 0:\n",
    "        k = min(int(stock_p2_inter), 5)\n",
    "        if k > 0:\n",
    "            return 99 + k  # action P2_STEP2 (k)\n",
    "\n",
    "    # Règle 3 : si M1 libre, demande P2 > 0 et MP dispo → STEP1 P2\n",
    "    if m1_free and demande_p2 > 0 and stock_raw > 0:\n",
    "        k = min(int(demande_p2), int(stock_raw), 5)\n",
    "        if k > 0:\n",
    "            return 49 + k  # action P2_STEP1 (k)\n",
    "\n",
    "    # Règle 4 : sinon, si M1 libre et demande P1 > 0 → produire P1\n",
    "    if m1_free and demande_p1 > 0 and stock_raw > 0:\n",
    "        k = min(max(int(demande_p1), 1), int(stock_raw), 5)\n",
    "        return k - 1       # action P1 (k = action+1)\n",
    "\n",
    "    # Règle 5 : sinon WAIT\n",
    "    return 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61926e71-846c-46d2-9411-91a017edb72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 3 — Politique experte masquée + fonction d'épisode expert\n",
    "\n",
    "def expert_policy_masked(env: WorkshopEnv, obs: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Version masquée de la politique experte :\n",
    "    - récupère le mask via env.get_action_mask()\n",
    "    - propose une action via expert_policy(obs, env)\n",
    "    - si l'action est invalide, on la remplace par une action valide\n",
    "    \"\"\"\n",
    "\n",
    "    mask = env.get_action_mask()  # bool array de taille 201\n",
    "    a = expert_policy(obs, env)   # <<< on passe maintenant env à la politique experte\n",
    "\n",
    "    # Si l'action proposée est invalide, on corrige\n",
    "    if not mask[a]:\n",
    "        # Priorité : WAIT si autorisé (200)\n",
    "        if mask[200]:\n",
    "            return 200\n",
    "        # Sinon, on prend la première action valide\n",
    "        valid_actions = np.nonzero(mask)[0]\n",
    "        if len(valid_actions) == 0:\n",
    "            # Cas pathologique : aucune action valide\n",
    "            return 200\n",
    "        return int(valid_actions[0])\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "def run_expert_episode(env: WorkshopEnv, max_steps: int = 10080):\n",
    "    \"\"\"\n",
    "    Joue un épisode complet avec l'expert masqué.\n",
    "    Renvoie :\n",
    "      - obs_array : (T, 23)  # 23 features normalisées\n",
    "      - act_array : (T,)\n",
    "      - total_reward\n",
    "      - nb_steps\n",
    "    \"\"\"\n",
    "    obs, info = env.reset()\n",
    "    obs_list = []\n",
    "    act_list = []\n",
    "    total_reward = 0.0\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        action = expert_policy_masked(env, obs)\n",
    "        obs_list.append(obs.copy())\n",
    "        act_list.append(action)\n",
    "\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    obs_array = np.stack(obs_list, axis=0)\n",
    "    act_array = np.array(act_list, dtype=np.int64)\n",
    "\n",
    "    return obs_array, act_array, total_reward, t + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80064d11-ada5-4466-aa9d-82955d9daf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Dataset expert initial =====\n",
      "obs shape : (10080, 23)\n",
      "actions shape : (10080,)\n",
      "Reward expert sur cet épisode : 12729.29999999846\n",
      "Steps joués : 10080\n",
      "\n",
      "Buffer DAgger initialisé :\n",
      "dagger_obs : (10080, 23)\n",
      "dagger_actions : (10080,)\n"
     ]
    }
   ],
   "source": [
    "# Cellule 4 — Génération du dataset expert initial\n",
    "\n",
    "env_expert = WorkshopEnv()\n",
    "expert_obs, expert_actions, R_expert, T_expert = run_expert_episode(env_expert)\n",
    "\n",
    "print(\"===== Dataset expert initial =====\")\n",
    "print(\"obs shape :\", expert_obs.shape)\n",
    "print(\"actions shape :\", expert_actions.shape)\n",
    "print(\"Reward expert sur cet épisode :\", R_expert)\n",
    "print(\"Steps joués :\", T_expert)\n",
    "\n",
    "# Initialisation du buffer DAgger\n",
    "dagger_obs = expert_obs.copy()\n",
    "dagger_actions = expert_actions.copy()\n",
    "\n",
    "print(\"\\nBuffer DAgger initialisé :\")\n",
    "print(\"dagger_obs :\", dagger_obs.shape)\n",
    "print(\"dagger_actions :\", dagger_actions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f4fd7fe-c177-46bc-a0ee-21a984fda5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Modèle élève initialisé (MaskablePPO + GPU + ActionMasker OK).\n"
     ]
    }
   ],
   "source": [
    "# Cellule 5 — Environnement avec ActionMasker + Modèle MaskablePPO\n",
    "\n",
    "import gymnasium as gym\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "\n",
    "# 1) Fonction de masquage (appelée automatiquement par ActionMasker)\n",
    "def mask_fn(env):\n",
    "    return env.get_action_mask()\n",
    "\n",
    "# 2) Environnement enveloppé\n",
    "env_student = ActionMasker(WorkshopEnv(), mask_fn)\n",
    "\n",
    "# 3) Modèle MaskablePPO\n",
    "from sb3_contrib import MaskablePPO\n",
    "\n",
    "model_student = MaskablePPO(\n",
    "    policy=\"MlpPolicy\",          # IMPORTANT : pas MultiInputPolicy\n",
    "    env=env_student,\n",
    "    verbose=1,\n",
    "    device=\"cuda\",\n",
    "    learning_rate=3e-4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    n_steps=4096,\n",
    "    batch_size=512,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.01,\n",
    "    max_grad_norm=0.5,\n",
    "    tensorboard_log=\"./tb_dagger_hybrid\"\n",
    ")\n",
    "\n",
    "print(\"Modèle élève initialisé (MaskablePPO + GPU + ActionMasker OK).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bdc9a37-1efa-4995-96fa-ced523ddbbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELLULE 6 — Entraînement supervisé de l'élève (corrigée)\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_student_supervised(model, obs_array, act_array, epochs=5, batch_size=512):\n",
    "    \"\"\"\n",
    "    Entraînement supervisé : l'élève imite l'expert sur tout le buffer DAgger.\n",
    "    - obs_array shape = (N, 23)\n",
    "    - act_array shape = (N,)\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    policy = model.policy.to(device)\n",
    "\n",
    "    X = torch.tensor(obs_array, dtype=torch.float32, device=device)\n",
    "    y = torch.tensor(act_array, dtype=torch.long, device=device)\n",
    "\n",
    "    # Sécurité : empêcher tout label hors [0,200]\n",
    "    y = torch.clamp(y, 0, policy.action_space.n - 1)\n",
    "\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=1e-4)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    N = len(X)\n",
    "    nb_batches = (N + batch_size - 1) // batch_size\n",
    "\n",
    "    print(f\"  → Entraînement supervisé sur {N} exemples ({nb_batches} batches)\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        perm = torch.randperm(N)\n",
    "        Xb = X[perm]\n",
    "        yb = y[perm]\n",
    "\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for i in range(nb_batches):\n",
    "            start = i * batch_size\n",
    "            end = min(start + batch_size, N)\n",
    "\n",
    "            xb_i = Xb[start:end]\n",
    "            yb_i = yb[start:end]\n",
    "\n",
    "            # Extraction SB3 correcte\n",
    "            features = policy.extract_features(xb_i)\n",
    "            pi_latent, _ = policy.mlp_extractor(features)\n",
    "            logits = policy.action_net(pi_latent)\n",
    "\n",
    "            loss = loss_fn(logits, yb_i)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"    Epoch {epoch+1}/{epochs} — loss = {total_loss/nb_batches:.4f}\")\n",
    "\n",
    "    print(\"  ✓ Entraînement supervisé terminé.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9cc816b-de85-454d-ac96-403a90b1418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 7 — Test de l'élève sur une semaine (version MlpPolicy)\n",
    "\n",
    "def run_student_episode(env, model, max_steps: int = 10080):\n",
    "    obs, info = env.reset()\n",
    "    total_reward = 0.0\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        # On donne simplement l'observation brute au modèle\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    return total_reward, t + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7b25bf2-537a-4335-b6cf-96375ab53aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELLULE 8 — Collecte des données DAgger \n",
    "# ============================================================\n",
    "\n",
    "def collect_dagger_data_from_student(model, max_steps=10080):\n",
    "    \"\"\"\n",
    "    L'élève joue un épisode complet.\n",
    "    L'expert corrige chaque action.\n",
    "    On renvoie :\n",
    "    - obs_list  : toutes les observations rencontrées\n",
    "    - act_list  : actions expertes correspondantes\n",
    "    - total_reward_student : reward cumulé du student\n",
    "    - nb_steps_student     : nombre de steps joués\n",
    "    \"\"\"\n",
    "\n",
    "    env = WorkshopEnv()\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    obs_list = []\n",
    "    act_list = []\n",
    "\n",
    "    total_reward = 0.0\n",
    "\n",
    "    for t in range(max_steps):\n",
    "\n",
    "        #  Très important : récupérer le MASQUE\n",
    "        mask = env.get_action_mask()\n",
    "\n",
    "        #  predict() DOIT recevoir action_masks pour éviter actions hors borne\n",
    "        action_student, _ = model.predict(\n",
    "            obs,\n",
    "            deterministic=True,\n",
    "            action_masks=mask\n",
    "        )\n",
    "\n",
    "        # L'expert corrige l'action\n",
    "        action_expert = expert_policy_masked(env, obs)\n",
    "\n",
    "        obs_list.append(obs.copy())\n",
    "        act_list.append(action_expert)\n",
    "\n",
    "        obs, reward, terminated, truncated, info = env.step(action_student)\n",
    "        total_reward += reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    return (\n",
    "        np.array(obs_list, dtype=np.float32),\n",
    "        np.array(act_list, dtype=np.int64),\n",
    "        float(total_reward),\n",
    "        t + 1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38933a77-7d01-47d7-a4dc-74ade72d09a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 9 — Une itération DAgger imitation-seule (SANS PPO)\n",
    "\n",
    "def dagger_hybrid_iteration(\n",
    "    model,\n",
    "    dagger_obs,\n",
    "    dagger_actions,\n",
    "    supervised_epochs: int = 3,\n",
    "    rl_timesteps: int = 10_000\n",
    "):\n",
    "    \"\"\"\n",
    "    VERSION DIAGNOSTIC : imitation supervisée SEULE\n",
    "    (on désactive PPO pour voir si le modèle apprend réellement les actions expertes)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n===== Phase 1 : Imitation supervisée sur buffer DAgger =====\")\n",
    "    train_student_supervised(model, dagger_obs, dagger_actions, epochs=supervised_epochs)\n",
    "\n",
    "    print(\"\\n===== Phase 2 : (désactivée) =====\")\n",
    "    print(\"⚠ PPO désactivé volontairement pour test de diagnostic.\")\n",
    "\n",
    "    print(\"\\n===== Phase 3 : Collecte DAgger (élève + expert) =====\")\n",
    "    new_obs, new_actions, R_student, steps_student = collect_dagger_data_from_student(model)\n",
    "\n",
    "    print(f\"Reward élève pendant collecte DAgger : {R_student:.2f} sur {steps_student} steps\")\n",
    "\n",
    "    # Agrégation au buffer\n",
    "    dagger_obs = np.vstack([dagger_obs, new_obs])\n",
    "    dagger_actions = np.concatenate([dagger_actions, new_actions])\n",
    "\n",
    "    print(\"Taille du buffer DAgger après agrégation :\", dagger_obs.shape)\n",
    "\n",
    "    return dagger_obs, dagger_actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c13db2a8-1287-4e36-b529-702887fd2fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================ DAgger Hybride - Itération 1/2 ================\n",
      "\n",
      "===== Phase 1 : Imitation supervisée sur buffer DAgger =====\n",
      "  → Entraînement supervisé sur 10080 exemples (20 batches)\n",
      "    Epoch 1/3 — loss = 5.2627\n",
      "    Epoch 2/3 — loss = 5.1704\n",
      "    Epoch 3/3 — loss = 5.0473\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "===== Phase 2 : (désactivée) =====\n",
      "⚠ PPO désactivé volontairement pour test de diagnostic.\n",
      "\n",
      "===== Phase 3 : Collecte DAgger (élève + expert) =====\n",
      "Reward élève pendant collecte DAgger : -5835.66 sur 10080 steps\n",
      "Taille du buffer DAgger après agrégation : (20160, 23)\n",
      "\n",
      ">>> Évaluation élève après itération 1 :\n",
      "    Reward sur 7 jours : -6309.72\n",
      "    Reward moyen par jour : -901.39\n",
      "    Steps joués : 10080\n",
      "    Modèle sauvegardé dans : maskedppo_dagger_iter_1.zip\n",
      "\n",
      "\n",
      "================ DAgger Hybride - Itération 2/2 ================\n",
      "\n",
      "===== Phase 1 : Imitation supervisée sur buffer DAgger =====\n",
      "  → Entraînement supervisé sur 20160 exemples (40 batches)\n",
      "    Epoch 1/3 — loss = 4.9572\n",
      "    Epoch 2/3 — loss = 4.6194\n",
      "    Epoch 3/3 — loss = 4.1639\n",
      "  ✓ Entraînement supervisé terminé.\n",
      "\n",
      "===== Phase 2 : (désactivée) =====\n",
      "⚠ PPO désactivé volontairement pour test de diagnostic.\n",
      "\n",
      "===== Phase 3 : Collecte DAgger (élève + expert) =====\n",
      "Reward élève pendant collecte DAgger : -5613.00 sur 10080 steps\n",
      "Taille du buffer DAgger après agrégation : (30240, 23)\n",
      "\n",
      ">>> Évaluation élève après itération 2 :\n",
      "    Reward sur 7 jours : -5934.70\n",
      "    Reward moyen par jour : -847.81\n",
      "    Steps joués : 10080\n",
      "    Modèle sauvegardé dans : maskedppo_dagger_iter_2.zip\n"
     ]
    }
   ],
   "source": [
    "# Cellule 10 — Boucle principale DAgger hybride + suivi des récompenses\n",
    "\n",
    "N_ITERS = 2           # nombre d'itérations DAgger\n",
    "SUPERVISED_EPOCHS = 3\n",
    "RL_TIMESTEPS = 10_000\n",
    "\n",
    "for it in range(1, N_ITERS + 1):\n",
    "    print(f\"\\n\\n================ DAgger Hybride - Itération {it}/{N_ITERS} ================\")\n",
    "\n",
    "    dagger_obs, dagger_actions = dagger_hybrid_iteration(\n",
    "        model_student,\n",
    "        dagger_obs,\n",
    "        dagger_actions,\n",
    "        supervised_epochs=SUPERVISED_EPOCHS,\n",
    "        rl_timesteps=RL_TIMESTEPS\n",
    "    )\n",
    "\n",
    "    # Test de l'élève sur une semaine\n",
    "    env_eval = WorkshopEnv()\n",
    "    reward_eval, steps_eval = run_student_episode(env_eval, model_student)\n",
    "\n",
    "    print(f\"\\n>>> Évaluation élève après itération {it} :\")\n",
    "    print(f\"    Reward sur 7 jours : {reward_eval:.2f}\")\n",
    "    print(f\"    Reward moyen par jour : {reward_eval / 7:.2f}\")\n",
    "    print(f\"    Steps joués : {steps_eval}\")\n",
    "\n",
    "    # Sauvegarde du modèle\n",
    "    model_path = f\"maskedppo_dagger_iter_{it}.zip\"\n",
    "    model_student.save(model_path)\n",
    "    print(f\"    Modèle sauvegardé dans : {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078b35ef-df55-47d9-a831-e864f81330c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================== PPO FINAL — Phase RL pure =====================\n",
      "\n",
      ">>> Début entraînement PPO final...\n"
     ]
    }
   ],
   "source": [
    "# Cellule 11 — PPO final après DAgger (entraînement pur RL)\n",
    "\n",
    "print(\"\\n===================== PPO FINAL — Phase RL pure =====================\\n\")\n",
    "\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "\n",
    "def mask_fn(env):\n",
    "    return env.get_action_mask()\n",
    "\n",
    "# Environnement pour PPO final\n",
    "env_rl = ActionMasker(WorkshopEnv(), mask_fn)\n",
    "\n",
    "model_rl = MaskablePPO(\n",
    "    policy=\"MlpPolicy\",  # MlpPolicy fonctionne parfaitement avec 23 features\n",
    "    env=env_rl,\n",
    "    verbose=0,\n",
    "    device=\"cuda\",\n",
    "    learning_rate=3e-4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    n_steps=4096,\n",
    "    batch_size=512,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.01,\n",
    "    max_grad_norm=0.5,\n",
    "    tensorboard_log=\"./ppo_final_log/\"\n",
    ")\n",
    "\n",
    "TOTAL_TIMESTEPS = 300_000  # selon GPU\n",
    "\n",
    "print(\">>> Début entraînement PPO final...\")\n",
    "model_rl.learn(total_timesteps=TOTAL_TIMESTEPS)\n",
    "print(\">>> Fin entraînement PPO final.\")\n",
    "\n",
    "model_rl.save(\"ppo_final_agent.zip\")\n",
    "print(\"Modèle PPO final sauvegardé.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d19c089-8029-4b6b-ab1d-df033b905176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 12\n",
    "\n",
    "# ===================== TEST FINAL PPO (Cellule autonome) =====================\n",
    "\n",
    "import numpy as np\n",
    "from sb3_contrib import MaskablePPO\n",
    "from env.workshop_env import WorkshopEnv\n",
    "\n",
    "print(\"\\n===================== TEST FINAL PPO =====================\\n\")\n",
    "\n",
    "# 1) Chargement du modèle entraîné\n",
    "model_rl = MaskablePPO.load(\"ppo_final_agent.zip\")\n",
    "\n",
    "# 2) Création d'un environnement nu pour le test\n",
    "env_test = WorkshopEnv()\n",
    "obs, info = env_test.reset()\n",
    "\n",
    "total_reward = 0.0\n",
    "\n",
    "# 3) Boucle d'évaluation sur 7 jours (10080 minutes)\n",
    "for t in range(10080):\n",
    "    mask = env_test.get_action_mask()\n",
    "\n",
    "    action, _ = model_rl.predict(\n",
    "        obs,\n",
    "        deterministic=True,\n",
    "        action_masks=mask     # <<< IMPORTANT !!!\n",
    "    )\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env_test.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "# 4) Résultats finaux\n",
    "print(\"Reward total PPO :\", total_reward)\n",
    "print(\"Reward moyen par jour :\", total_reward / 7)\n",
    "print(\"Steps joués :\", t + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f037d42-c27c-4612-a102-ffe396dcb3d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:qlearning]",
   "language": "python",
   "name": "conda-env-qlearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
